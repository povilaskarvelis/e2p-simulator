<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="Getting started with the Effect Size Calculator - A guide for understanding and using the tool">
    <meta name="viewport" content="width=1400, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Get Started - E2P Simulator</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VDYP5LLPT5"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VDYP5LLPT5');
    </script>
    
    <!-- Load libraries first -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.1/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <!-- Load styles -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/binary.css">
    <link rel="stylesheet" href="css/continuous.css">
    
    <!-- MathJax for professional equation rendering -->
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.auto.min.js"></script>
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
        },
        svg: {
            scale: 1,
            minScale: .5,
            mtextInheritFont: false,
            merrorInheritFont: true,
            mathmlSpacing: false,
            skipAttributes: {},
            exFactor: .5,
            displayAlign: 'center',
            displayIndent: '0',
            versionWarning: false,
        },
        options: {
            enableMenu: false,
            menuOptions: {
                settings: {
                    zoom: "Click",
                    zscale: "200%",
                }
            }
        }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Header -->
    <header class="site-header">
        <a href="index.html" class="site-title">E2P Simulator</a>
        <nav class="site-nav">
            <a href="index.html" class="nav-link">Home</a>
            <a href="get-started.html" class="nav-link active">Get Started</a>
        </nav>
    </header>

    <h1>Getting Started with E2P Simulator</h1>

    <div class="description">
        <p>Welcome to the E2P Simulator! This guide will help you understand how to use this tool to explore 
           the relationship between statistical effect sizes and their practical predictive utility.</p>
    </div>

    <div class="guide-section">
        <h2>What is the E2P Simulator?</h2>
        <p>The E2P Simulator (Effect-to-Prediction Simulator) bridges the gap between statistical effect sizes and practical utility. It interactively and visually shows how commonly reported effect size metrics (like Cohen's d and Pearson's r) translate into real-world predictive performance, while accounting for often overlooked factors: measurement reliability and outcome base rates.</p>

        <p>The E2P Simulator has several potential applications:</p>
        
        <ul>
            <li><strong>Interpretation of findings</strong>: It helps researchers move beyond arbitrary "small/medium/large" labels and ground the interpretation of their findings in terms of actual predictive value in a given context (e.g., prevalence of the outcome, measurement reliability).</li>
            <li><strong>Research planning</strong>: By clearly demonstrating how effect sizes relate to practical utility and how they're influenced by measurement reliability and outcome base rates, researchers can develop more realistic research plans and allocate resources more effectively.</li>
            <li><strong>Education</strong>: The simulator's interactive design serves as an excellent teaching tool, helping students and researchers understand the connection between abstract statistical metrics and their practical implications in a more intuitive way.</li>
        </ul>
        
    </div>

    <div class="guide-section">
        <h2>Key Features</h2>

        <h3>Binary vs. Continuous Outcomes</h3>
        
        <p>The E2P Simulator offers two complementary analysis modes: when outcomes are binary and when outcomes are continuous (in machine learning language, that corresponds to classification and prediction tasks, respectively). Each mode includes specialized calculators to help you translate between effect sizes and practical applications.</p>
        
        <ul>
            <li><strong>Binary Outcomes</strong>: Use this mode when your research involves naturally distinct groups that exist independently of your measurement (e.g., treatment vs. control, male vs. female).</li>
            
            <li><strong>Continuous Outcomes</strong>: Use this mode when your research involves continuous variables that are later thresholded to create groups (e.g., responders vs. non-responders to a treatment, above/below cutoff on a risk scale).</li>
        </ul>
        
        <h3>Measurement Reliability and True vs. Observed Effects</h3>
        <p>All measurements contain some degree of error, which attenuates the effect sizes we observe in practice. The simulator allows you to explore this relationship by adjusting the reliability of the measurements and toggling between true effect sizes (what would be observed with perfect measurement) and observed effect sizes (what you actually see given a specific level of reliability).</p>
            
        <p>Note that the simulator does not account for sample size limitations, which can introduce additional uncertainty around the true effect size through sampling error.</p>
        
        <h3>Base Rates</h3>
        <p>The base rate (prevalence) of outcomes dramatically affects predictive utility. Even with strong effect sizes, rare outcomes (e.g, suicide attempts) would be very difficult to predict with high precision. The simulator demonstrates how the same effect size can yield very different predictive performance depending on base rates.</p>
            
        <p>Note that base rate considerations here are not the same as having imbalanced classes in machine learning, which merely refers to unequal group sizes in collected data. Instead, here we want to consider real-world prevalence, which offers valuable insights into how different effect sizes translate into practical utility in real-world settings.</p>

    </div>

    <div class="guide-section">
        <h2 id="understanding-metrics">Understanding Predictive Metrics</h2>
        
        <h3>Classification Outcomes and Metrics</h3>
        <p>When using a predictor to classify cases into two groups, there are four possible outcomes: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). These form the basis for all predictive metrics.</p>
        
        <div id="predictive-metrics-diagram" style="text-align: center; margin: 20px 0;">
            <img src="images/prediction_metrics.png" alt="Prediction metrics diagram showing confusion matrix and all derived metrics" class="screenshot" style="max-width: 90%;">
        </div>
        
        <p>The image above illustrates how these four outcomes relate to various classification metrics. On the left, you can see how negative (e.g., controls) and positive (e.g., cases) distributions overlap and how a classification threshold (red line) creates these four outcomes. On the right, you'll find the confusion matrix and the formulas for key metrics derived from it. Note how some metrics have multiple names (e.g., Sensitivity/Recall/TPR, Precision/PPV) - this reflects how the same concepts are referred to differently across fields like medicine, cognitive science, and machine learning.</p>
        
        <h3>Key Metrics for Different Contexts</h3>
        <p>Depending on your research context, certain metrics may be more relevant than others:</p>
        <ul>
            <li><strong>When false negatives are costly</strong> (e.g., missing a disease diagnosis): Focus on sensitivity/recall</li>
            <li><strong>When false positives are costly</strong> (e.g., unnecessary treatments): Focus on specificity</li>
            <li><strong>When you need to know how much to trust a positive result</strong>: Focus on precision/PPV</li>
            <li><strong>When working with rare outcomes</strong>: Consider balanced accuracy, F1 score, or MCC</li>
        </ul>
        
        <h3>Threshold-Independent Metrics</h3>
        <p>Some metrics evaluate performance across all possible thresholds:</p>
        <ul>
            <li><strong>AUC (Area Under the ROC Curve)</strong>: Visualizes how well a model balances true positives (sensitivity) and false positives (1-specificity) across all possible thresholds, capturing the trade-off between the two. An AUC of 0.5 means the model is no better than flipping a coin, while 1.0 means perfect separation between groups.</li>
            <li><strong>PR-AUC (Area Under the Precision-Recall Curve)</strong>: Shows how well a model can maintain both precision (PPV) and recall (sensitivity) together, which is particularly informative in the context of rare outcomes, as the base rate affects precision. A larger PR-AUC means the model can achieve high precision without sacrificing recall (or vice versa), indicating a smaller trade-off between finding all positive cases and avoiding false alarms.</li>
        </ul>
       </div>

    <div class="guide-section">

        <h2>Understanding Multivariate Effects with Calculators</h2>
        <p>Both binary and continuous outomes analysis modes include calculators that help explore how multiple predictors can be combined to achieve stronger effects:</p>
        
        <ul>
            <li><strong>Mahalanobis D Calculator</strong> (Binary mode)</li>
            <li><strong>Multivariate R² Calculator</strong> (Continuous mode)</li>
        </ul>
        
        <p>The calculators can help approximate the expected performance of multivariate models without having to train the full models and thus helps with research planning and model development. More specifically, they illustrate:</p>
        
        <ul>
            <li>How the number of predictors affects combined effect size</li>
            <li>The diminishing returns of adding more predictors</li>
            <li>How collinearity (correlation among predictors) reduces their combined effectiveness</li>
            <li>The trade-offs between using fewer strong predictors versus more moderate ones</li>
        </ul>     

        <h3>Calculator Assumptions and Limitations</h3>
        <p>Both calculators correspond to fundamental predictive models in statistics: the Mahalanobis D Calculator approximates Linear Discriminant Analysis and logistic regression, while the Multivariate R² Calculator aligns with multiple linear regression using standardized variables. These connections make the calculators valuable for understanding how these common predictive methods work in practice.</p>
        
        <p>Like the statistical methods they approximate, the calculators operate under several simplifying assumptions:</p>
        <ul>
            <li><strong>Average effects and correlations</strong>: The calculators use single values to represent the average effect size across predictors and average correlation among them, which can provide useful approximations even when individual predictors vary in strength</li>
            <li><strong>Linear, additive effects</strong>: The formulas assume that predictors contribute independently to the overall effect in a simple additive way. They don't account for interaction effects, where the impact of one predictor depends on the level of another predictor</li>
            <li><strong>Normality</strong>: Variables are assumed to be normally distributed</li>
        </ul>
        
        <p>Despite these limitations, these calculators serve as valuable tools for building intuition about how multiple predictors combine to achieve stronger effects. Even though real-world predictors will vary in their individual strengths and correlations, the overall patterns demonstrated (such as diminishing returns and the impact of shared variance) remain informative for understanding multivariate relationships.</p>
    </div>

    <div class="guide-section">
        <h2>Quick Start Examples</h2>
        
        <h3>Example 1: Predicting Depression Diagnosis</h3>
        <p>A psychiatrist wants to understand how well a biomarker can predict depression diagnosis:</p>
        <ol>
            <li>Select Binary Mode and set base rate to 0.1 (~10% of the population being depressed)</li>
            <li>Set the grouping reliability to 0.37 (empirical value for DSM-5 depression diagnosis)</li>
            <li>Set the predictor reliability to 0.7 (an optimistic value for clinical biomarkers)</li>

            <li>Set the effect size to d = 0.8 (a large effect size that is optimistic and rarely seen in practice):
                <ul>
                    <li>This yields AUC = 0.71 and PR-AUC = 0.23, showing limited discriminative ability despite being considered a "large" effect</li>
                    <li>Even with this optimistic effect size, the predictive utility remains very modest, especially for when it comes to the tradeoff between recall and precision (as shown by the low PR-AUC)</li>
                </ul>
            </li>

            <li>Note that with the low reliability values (0.37 for diagnosis, 0.7 for biomarker), this observed effect corresponds to a much larger true effect, d = 1.57, highlighting the improvements in predictive utility that can be achieved by simply improving measurement reliability.</li>

            <li>Use the Mahalanobis D calculator to explore how many biomarkers with smaller d values could be combined to achieve the same discriminative power as a single biomarker with d = 2.0 or larger</li>
        </ol>
        
        <h3>Example 2: Interpreting a Correlation</h3>
        <p>A researcher has found r = 0.4 between a biomarker and treatment response in depression:</p>
        <ol>
            <li>Select Continuous Mode and set reliability to 0.7 (aspirational for clinical biomarkers and outcomes)</li>
            <li>Set the base rate to 30% (response rate to antidepressant treatment)</li>
            <li>Observe that despite being a "large" correlation rarely seen in practice:
                <ul>
                    <li>This correlation yields AUC = 0.69 and PR-AUC = 0.47 for distinguishing cases above/below thresholds, indicating rather poor predictive utility</li>
                    <li>Consider how improving reliability could substantially increase predictive utility</li>
                </ul>
            </li>
        </ol>
    </div>

    <!-- Footer -->
    <footer style="text-align: center; padding: 20px; margin-top: 40px; border-top: 1px solid #eee;">
        <p style="color: #666; font-size: 0.9em;">
            E2P Simulator is open source software. 
            <a href="https://github.com/povilaskarvelis/e2p-simulator" style="color: #0366d6; text-decoration: none;">View on GitHub</a> | 
            <a href="https://github.com/povilaskarvelis/e2p-simulator/blob/main/LICENSE" style="color: #0366d6; text-decoration: none;">MIT License</a>
        </p>
        <p style="color: #666; font-size: 0.9em; margin-top: 10px;">
            Created by <a href="https://povilaskarvelis.github.io/" style="color: #0366d6; text-decoration: none;" target="_blank">Povilas Karvelis</a>
        </p>
    </footer>

    <!-- Load scripts -->
    <script src="js/utils.js"></script>
    <script src="js/main.js"></script>
</body>
</html> 