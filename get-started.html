<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="Getting started with E2P Simulator">
    <meta name="viewport" content="width=1400, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>E2P Simulator | Get Started</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VDYP5LLPT5"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VDYP5LLPT5');
    </script>
    
    <!-- Favicon links -->
    <link rel="icon" href="images/favicon.png" type="image/png">
    <link rel="apple-touch-icon" href="images/favicon.png">
    <link rel="canonical" href="https://e2p-simulator.com/get-started.html" />
    
    <!-- MathJax handles mathematical rendering, no other libraries needed for documentation -->
    
    <!-- Load styles -->
    <link rel="stylesheet" href="css/base.css">
    
    <!-- MathJax for professional equation rendering -->
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.auto.min.js"></script>
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
        },
        svg: {
            scale: 1,
            minScale: .5,
            mtextInheritFont: false,
            merrorInheritFont: true,
            mathmlSpacing: false,
            skipAttributes: {},
            exFactor: .5,
            displayAlign: 'center',
            displayIndent: '0',
            versionWarning: false,
        },
        options: {
            enableMenu: false,
            menuOptions: {
                settings: {
                    zoom: "Click",
                    zscale: "200%",
                }
            }
        }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="get-started-page">
    <!-- Header -->
    <header class="site-header">
        <a href="index.html" class="site-title" style="display: inline-flex; align-items: center;">
            <img src="images/favicon.png" alt="E2P Simulator Logo" style="height: 32px; width: 32px; margin-right: 8px;">E2P Simulator
        </a>
        <div class="header-right">
            <nav class="site-nav">
                <a href="index.html" class="nav-link">Home</a>
                <a href="get-started.html" class="nav-link active">Get Started</a>
            </nav>
            <a href="https://github.com/povilaskarvelis/e2p-simulator" target="_blank" rel="noopener noreferrer" class="github-link" aria-label="View source on GitHub">
                <svg viewBox="0 0 98 96" xmlns="http://www.w3.org/2000/svg" class="github-logo"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-2.958.324-2.958 4.93 1.364 7.496 5.032 7.496 5.032 4.38 7.849 11.564 5.571 14.355 4.239.446-3.281 1.711-5.571 3.074-6.84-10.943-1.243-22.478-5.526-22.478-24.283 0-5.378 1.94-9.778 5.021-13.2-.485-1.243-2.11-6.283.484-13.018 0 0 4.125-1.304 13.426 5.032a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.336 13.427-5.032 13.427-5.032 2.594 6.735.97 11.775.486 13.018 3.14 3.422 5.021 8.012 5.021 13.2 0 18.807-11.478 23.015-22.478 24.283 1.79 1.548 3.316 4.481 3.316 9.126 0 6.608-.056 11.897-.056 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="#24292f"/></svg>
            </a>
        </div>
    </header>

    <h1>Getting Started with E2P Simulator</h1>

    <div class="description">
        <p>Welcome to E2P Simulator! This guide will help you understand what it does, <br> why it is needed, and how to use it.</p>
    </div>

    <!-- Sticky Sidebar Table of Contents -->
    <nav class="toc-sidebar" id="toc-sidebar">
        <h3>Contents</h3>
        <ol>
            <li><a href="#what-is">What is E2P Simulator?</a></li>
            <li><a href="#why-needed">Why is it needed?</a></li>
            <li><a href="#how-to-use">How to use</a></li>
            <li><a href="#understanding-metrics">Predictive Metrics</a></li>
            <li><a href="#predictive-utility-analysis">Predictive Utility Analysis</a></li>
            <li><a href="#examples">Examples</a></li>
            <li><a href="#sample-size">Sample Size</a></li>
            <li><a href="#feedback">Feedback</a></li>
            <li><a href="#references">References</a></li>
        </ol>
    </nav>

    <div class="guide-section">
        <h2 id="what-is">What is E2P Simulator?</h2>
        <p>E2P Simulator (Effect-to-Prediction Simulator) is an interactive open-source tool that allows researchers to visually and quantitatively explore the relationships between effect sizes (e.g., Cohen's d, Pearson's r), discriminative ability (e.g., ROC-AUC, sensitivity, specificity), predictive value (e.g., PPV, NPV, PR-AUC), and clinical utility (Net Benefit), while accounting for measurement reliability and outcome base rates (<a href="https://doi.org/10.21105/joss.08334" target="_blank">Karvelis & Diaconescu, 2025</a>).</p>

        <p>As such, it provides an easy way to perform <em><a href="#predictive-utility-analysis">predictive utility analysis</a></em>: estimating how research findings will translate into real-world prediction or what effect sizes or discriminative ability are needed to achieve a desired level of predictive value and clinical utility. Similar to how power analysis tools (such as <a href="https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower" target="_blank">G*Power</a>) help researchers plan for statistical significance, E2P Simulator helps plan for practical significance.</p>

        <p>E2P Simulator has several key applications:</p>
        
        <ul>
            <li><strong>Interpretation of findings</strong>: It helps researchers move beyond arbitrary "small/medium/large" effect size labels and misleading predictive metrics by grounding their interpretation in estimated real-world predictive utility.</li>
            <li><strong>Research planning</strong>: Being able to easily derive what effect sizes and predictive performance are needed to achieve a desired real-world predictive performance allows researchers to plan their studies more effectively and allocate resources more efficiently.</li>
            <li><strong>Education</strong>: The simulator's interactive design makes it a valuable teaching tool, helping researchers develop a more intuitive understanding of how different abstract statistical metrics relate to one another and to real-world utility.</li>
            <li><strong>Reporting standardization</strong>: E2P Simulator can help standardize transparent reporting practices by providing a complete view of the all key metrics as well as the underlying data distribution.</li>
        </ul>
    </div>

    <div class="guide-section">
        <h2 id="why-needed">Why is E2P Simulator needed?</h2>
        
        <p>Many research areas such as biomedical, behavioral, education, and sports sciences are increasingly studying individual differences to build predictive models to personalize treatments, learning, and training. Identifying reliable biomarkers and other predictors is central to these efforts. Yet, several entrenched research practices continue to undermine the search for predictors:</p>

        <ul>
            <li><strong>Difficulty interpreting effect sizes:</strong> The interpretation of effect sizes, which are critical for gauging real-world utility, is often reduced to arbitrary cutoffs (small/medium/large) without conveying their practical utility.</li>
            <li><strong>Overlooked measurement reliability:</strong> Measurement noise attenuates both effect sizes and predictive performance, yet it is rarely accounted for in study design or interpretation of findings.</li>
            <li><strong>Neglected outcome base rate:</strong> Low base rate of outcomes can drastically limit predictive performance in real-world settings, but is rarely accounted for when evaluating the translational potential of prediction models.</li>
        </ul>

        <p>Together, these issues undermine the quality and impact of academic research, because routinely reported metrics do not reflect real-world utility. Whether researchers focus on achieving statistical significance of individual predictors or optimizing model performance metrics like accuracy and ROC-AUC, both approaches often lead to unrealistic expectations about practical impact. In turn, this results in inefficient study planning, resource misallocation, and considerable waste of time and funding.</p>

        <p>E2P Simulator is designed to address these fundamental challenges by placing measurement reliability and outcome base rate at the center of study planning and interpretation. It helps researchers understand how these factors jointly shape real-world predictive utility, and guides them in making more informed research decisions. For a detailed analysis of these issues in the context of psychiatry research, see <a href="https://osf.io/preprints/psyarxiv/atn5m_v1" target="_blank">Karvelis et al. (2025)</a>.</p>

    <div class="guide-section">
        <h2 id="how-to-use">How to use E2P Simulator</h2>

        <p>E2P Simulator is designed to be intuitive and interactive. You can explore different scenarios by adjusting effect sizes, measurement reliability, base rate, and decision threshold, and immediately see how these changes impact predictive performance through the visualizations and metrics. Still, in this section we will highlight and clarify some of the key features and assumptions of the simulator.</p>

        <div id="use-summary-diagram" style="text-align: center; margin: 0;">
            <img src="images/use_summary.png" alt="E2P Simulator overview showing all key inputs and interactive elements at a high level" class="screenshot" style="max-width: 80%; width: 80%;">
        </div>
        
        <p>The image above provides an overview of all E2P Simulator's interactive components.</p>
  
        <h3>Binary vs. Continuous Outcomes</h3>
        <p>E2P Simulator provides two analysis modes that cover the two most common research scenarios:</p>

        <ul>
            <li><strong>Binary Mode</strong>: Considers dichotomous outcomes such as diagnostic categories (e.g., cases vs. controls) or discrete states (e.g., success vs. failure). All metric calculations and conversions in this mode are completely analytical and follow the formulas provided on the page.</li>
            
            <li><strong>Continuous Mode</strong>: Considers continuous measurements such as symptom severity or performance scores that may need to be categorized (e.g., responders vs. non-responders or performers vs. non-performers) for practical decisions. This mode is based on actual data simulations rather than analytical solutions, hence it may be slower to respond to inputs.</li>
        </ul>
        
        <h3>Measurement Reliability and True vs. Observed Effects</h3>
        <p>Measurement reliability attenuates observed effect sizes, which in turn reduces predictive performance. The simulator allows you to toggle between "true" effect sizes (what would be observed with perfect measurement) and "observed" effect sizes (what we actually see given imperfect reliability). The reliability of continuous variables (predictors or outcomes) is specified using the Intraclass Correlation Coefficient (ICC), which typically corresponds to test-retest reliability. For binary variables, reliability is specified using Cohen's kappa (κ), which usually represents inter-rater reliability.</p>
        
        <p>For continuous outcomes, where both the predictor and outcome are continuous, the relationship between true and observed Pearson's r is given by:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[r_{\text{observed}} = r_{\text{true}} \times \sqrt{ICC_{\text{predictor}} \times ICC_{\text{outcome}}}\]
        </div>

        <p>For binary outcomes, where a continuous predictor is used to classify a binary outcome, the relationship between true and observed Cohen's d is:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[d_{\text{observed}} = d_{\text{true}} \times \sqrt{\frac{2 \times ICC_1 \times ICC_2}{ICC_1 + ICC_2} \times \sin(\frac{\pi}{2} \kappa)}\]
        </div>

        <p>Here, \(ICC_1\) and \(ICC_2\) denote the reliability of the continuous predictor in each of the two outcome groups, and \(\kappa\) is the reliability of the binary outcome classification (e.g., interrater reliability of a diagnosis).</p>

        <p>See <a href="https://doi.org/10.3389/fpsyg.2025.1592658" target="_blank">Karvelis & Diaconescu (2025)</a> for more details on how reliability attenuates individual and group differences.</p>
       
        <p>Note that the simulator does not account for sample size limitations, which can introduce additional uncertainty around the true effect size through sampling error.</p>
        
        <h3>Base Rate</h3>
        <p>Base rate (also referred to as prevalence) refers to the proportion of individuals in the population who have the outcome of interest before considering any predictors or test results (in Bayesian terms, this is the prior probability of the outcome). To estimate real-world predictive utility, the base rate should be set to reflect the population where your predictor or model will actually be used — not the composition of your study sample. This distinction is crucial because research studies often use case-control designs with balanced sampling (e.g., 50% cases, 50% controls) that do not reflect real-world base rate. This is one of the most commonly overlooked problems in evaluating prediction models (<a href="https://doi.org/10.1007/978-3-030-50423-6_6" target="_blank">Brabec et al., 2020</a>), as the base rate directly affects multiple metrics used for model evaluation (see <a href="#understanding-metrics">Understanding Predictive Metrics</a>).</p>
            
        <p>For instance, if you are developing a model for a rare disorder that affects 2% of the general population, the base rate should be set to 2%, even if your training dataset contains equal numbers of cases and controls. However, if your model will be used in a pre-screened high-risk population where the disorder base rate is 20%, then 20% becomes the relevant base rate (however, in this scenario, the effect size should also reflect the difference between cases and high-risk controls rather than general population controls).</p>        

        <h3 id="multivariable">Multivariable Simulators</h3>

        <p>Both binary and continuous outcomes analysis modes include multivariable simulators that help estimate how many predictors with a given effect size need to be combined to achieve a desired level of predictive performance. These simulators can approximate the expected performance of multivariate models without having to train the full models, making them useful for research planning and model development. The simulators illustrate how increasing the number of predictors improves performance while also showing how collinearity leads to diminishing returns,demonstrating that small effects do not add up as quickly as many researchers might intuitively expect. </p>
        
        <p>For binary outcomes, the simulator displays ROC-AUC and PR-AUC. ROC-AUC provides a threshold-independent measure of discriminative ability while PR-AUC accounts for the base rate to reflect real-world performance with imbalanced outcomes. The conversion from multiple predictors to ROC-AUC is done via Mahalanobis D, a multivariate generalization of Cohen's d, which allows you to start from familiar effect size metrics.</p>
        
        <p>For continuous outcomes, the simulator displays total variance explained (R²) and PR-AUC. R² provides the standard measure of predictive performance in regression models, while PR-AUC accounts for the base rate to reflect real-world classification performance. The conversion from R² to PR-AUC is done by dichotomizing the continuous outcome at the base rate threshold, which allows you to assess how well the model would perform if used to make binary decisions (e.g., identifying the top 20% of patients most likely to benefit from treatment).</p>

        <h4>Assumptions and Limitations</h4>
        
        <p> The multivariable simulators are based on several simplifying assumptions:</p>
        <ul>
            <li><strong>Average effects and correlations</strong>: The simulators use single values to represent the average effect size across predictors and average correlation among them, which provides useful approximations for understanding general principles of multivariate relationships and expected model performance.</li>
            <li><strong>Linear effects</strong>: The formulas assume predictors contribute additively without interactions (where one predictor's effect depends on another). This assumption is supported by research showing that in clinical prediction, complex non-linear models generally do not outperform simple linear logistic regression (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0895435618310813" target="_blank">Christodoulou et al., 2019</a>). In general, more complex machine learning models excel at capturing non-linear relationships that we expect to see in the real world, but they also require more data and are more prone to overfitting.</li>
            <li><strong>Normality</strong>: The underlying variables are assumed to be normally distributed. This is consistent with the assumptions of input metrics like Cohen's d and Pearson's r, although in practice these are often computed despite normality violations.</li>
        </ul>
        
        <p>Even though real-world predictors will often not be normally distributed and will vary in their individual strengths and collinearity, the general trends (such as diminishing returns and the impact of shared variance among the predictors) remain informative for understanding multivariate relationships and estimating expected model performance.</p>

    </div>

    <div class="guide-section">
        <h2 id="understanding-metrics">Understanding Predictive Metrics</h2>
        
        <h3>Classification Outcomes and Metrics</h3>
        <p>When using a predictor to classify cases into two groups, there are four possible outcomes: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). These form the basis for all predictive metrics.</p>
        
        <div id="predictive-metrics-diagram" style="text-align: center; margin: 0 30px 0 0;">
            <img src="images/prediction_metrics.png" alt="Prediction metrics diagram showing confusion matrix and all derived metrics" class="screenshot" style="max-width: 80%;">
        </div>
        
        <p>The image above illustrates how these four outcomes are used to derive classification metrics. On the left, you can see how negative (e.g., controls) and positive (e.g., cases) distributions overlap and how a classification threshold (red line) creates these four outcomes. On the right, you can see the confusion matrix and the formulas for key metrics derived from it. Note how some metrics have multiple names (e.g., sensitivity/recall/TPR, precision/PPV) - this reflects how the same concepts are referred to differently across fields like medicine, cognitive science, and machine learning.</p>
        
        <p>A short summary of each metric:</p>
        <ul>
            <li><strong>Sensitivity (Recall, True Positive Rate)</strong>: Measures the proportion of actual positives correctly identified. Useful when missing a positive case is costly (e.g., disease screening), but ignores false positives.</li>
            <li><strong>Specificity (True Negative Rate)</strong>: Measures the proportion of actual negatives correctly identified. Important when false alarms are costly (e.g., confirming a diagnosis before a risky treatment), but ignores false negatives.</li>
            <li><strong>Accuracy</strong>: The proportion of all predictions that are correct. Intuitive but can be misleading for imbalanced datasets, as it is heavily influenced by the majority class.</li>
            <li><strong>Balanced Accuracy</strong>: The average of Sensitivity and Specificity. A better measure than accuracy for imbalanced datasets, but it gives equal weight to both types of errors and does not account for the base rate, which is critical for assessing real-world utility.</li>
            <li><strong>Positive Predictive Value (PPV, Precision)</strong>: The proportion of positive predictions that are actually correct. It informatively accounts for base rate, and corresponds to the posterior probability of a condition given a positive test result ("How likely is a positive prediction to be true?"). As such, it is a crucial metric for clinical decision-making.</li>
            <li><strong>Negative Predictive Value (NPV)</strong>: The proportion of negative predictions that are actually correct. It also informatively accounts for the base rate and corresponds to the posterior probability of not having a condition given a negative test result ("How likely is a negative prediction to be true?"). This makes it a crucial metric for ruling out conditions.</li>
            <li><strong>F1 Score</strong>: The harmonic mean of Precision and Recall. A useful summary when you need to balance finding all positives and not making too many false alarms, though it can be hard to interpret directly.</li>
            <li><strong>Matthews Correlation Coefficient (MCC)</strong>: A correlation between observed and predicted classifications. A balanced measure suitable for imbalanced datasets, but it is less intuitive to interpret and does not reveal the types of errors being made.</li>
        </ul>
        
        <h3 id="threshold-metrics">Threshold-Independent Metrics</h3>
        
        <div id="aucs-diagram" style="float: right; margin: 0 0 20px 20px;">
            <img src="images/aucs.png" alt="ROC and Precision-Recall curves showing AUC calculation" class="screenshot" style="max-width: 280px; width: 280px;">
        </div>
        
        <p>Some metrics evaluate performance across all possible thresholds and can serve as a better summary of the overall model performance. These include:</p>
        <ul>
            <li><strong>ROC-AUC (Area Under the Receiver Operating Characteristic Curve)</strong>: Summarizes how well a model balances true positives (sensitivity) and false positives (1-specificity) across all possible thresholds. ROC-AUC measures inherent discriminative capacity independent of base rates, which means it remains constant across different outcome base rates. This makes it a valuable metric for comparing models across different populations.</li>
            <li><strong>PR-AUC (Area Under the Precision-Recall Curve)</strong>: Summarizes how well a model balances precision (PPV) and recall (sensitivity) across all thresholds. Because precision is dependent on the base rate, PR-AUC can often be a more informative summary metric for real-world contexts with low base rates. The caveat is that for it to be truly informative, the base rate used to obtain PR-AUC should reflect expected real-world conditions (instead of being based on artificially balanced datasets).</li>
        </ul>
        
        <p>Both ROC-AUC and PR-AUC represent areas under their respective curves and are mathematically expressed as integrals:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[ROC\text{-}AUC = \int_0^1 TPR(FPR) \, d(FPR)\]
        </div>
    
        <div style="text-align: center; margin: 15px 0;">
            \[PR\text{-}AUC = \int_0^1 PPV(TPR) \, d(TPR)\]
        </div>
        <p>These integrals are computed using trapezoidal numerical integration.</p>
        
        <h3 id="dca-analysis">Decision Curve Analysis (DCA)</h3>
        <p>Decision Curve Analysis (<a href="https://pubmed.ncbi.nlm.nih.gov/17099194/" target="_blank">Vickers & Elkin, 2006</a>) evaluates the clinical utility of a predictive model or a single predictor by explicitly balancing the costs of false positives against the benefits of true positives.</p>
        
        <div id="dca-diagram" style="float: right; margin: 0 0 20px 20px;">
            <img src="images/dca.png" alt="Decision Curve Analysis example showing net benefit curves and shaded areas representing different strategies" class="screenshot" style="max-width: 320px; width: 320px;">
        </div>
        
        <p>A DCA plot typically includes three key curves:</p>
        <ul>
            <li><strong>Model</strong>: Shows the net benefit of using the model (or a single predictor) at different threshold probabilities. </li>
            <li><strong>All</strong>: Represents the strategy of intervening for everyone regardless of their predicted risk. This strategy maximizes sensitivity (no false negatives) but results in many unnecessary interventions (false positives).</li>
            <li><strong>None</strong>: Represents the strategy of intervening for no one, which always yields zero net benefit but avoids all intervention-related harms.</li>
        </ul>
        
        <p>The <strong>net benefit</strong> formula accounts for both the benefits of true positives and the costs of false positives:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[NB = \frac{TP}{N_{total}} - \frac{FP}{N_{total}} \times \frac{p_t}{1-p_t}\]
        </div>
        <p>Where N<sub>total</sub> is the total sample size, and p<sub>t</sub> is the threshold probability. It represents the minimum predicted probability of an outcome at which you would decide to intervene (e.g., diagnose or treat). For instance, if p<sub>t</sub> = 0.10, you would intervene for anyone with a predicted risk ≥ 10%. The choice of p<sub>t</sub> determines a specific balance between sensitivity (finding true cases) and specificity (avoiding false alarms). In practical terms, p<sub>t</sub> represents the trade-off between benefits and harms: for every 1/(1-p<sub>t</sub>) - 1 unnecessary interventions you are willing to accept to prevent one adverse outcome. The ratio p<sub>t</sub>/(1-p<sub>t</sub>) in the net benefit formula captures this relative weighting of false positives compared to true positives. The optimal p<sub>t</sub> can be estimated as:</p>

        <div style="text-align: center; margin: 15px 0;">
            \[p_t = \frac{C_{FP}}{C_{FP} + C_{FN}}\]
        </div>

        <p>Where C<sub>FP</sub> is the cost of a false positive (unnecessary intervention) and C<sub>FN</sub> is the cost of a false negative (missed positive case). p<sub>t</sub> can also be estimated through expert surveys, stakeholder preferences, or established guidelines.</p>
        
        <p>For population screening, p<sub>t</sub> is typically set low because missing true cases is costlier than unnecessary follow-ups, so more false positives are acceptable. For diagnostic confirmation (e.g., before initiating high-risk treatment), p<sub>t</sub> is set higher to avoid false positives, reflecting a preference for specificity. As a rule of thumb, screening scenarios may use p<sub>t</sub> in the 1–10% range, whereas diagnostic decisions often warrant much higher p<sub>t</sub> (for example 30–70% or more), depending on harms and preferences.</p>
        
        <p>What we often want to know is not the absolute NB, but added value. <strong>ΔNB (Delta Net Benefit)</strong> measures this additional utility by comparing the model against the better of the two simple strategies (either All or None) at a specific threshold probability:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[\Delta NB = NB_{\text{model}} - \max(NB_{\text{All}}, NB_{\text{None}})\]
        </div>
        
        <p>At each threshold probability, the model's net benefit is compared against whichever simple strategy performs better at that threshold. This provides a more conservative and meaningful assessment of the model's added value. A positive ΔNB indicates that the predictive model offers genuine improvement over the best simple strategy, while values near zero suggest that simple strategies may be equally effective.</p>

        <p>DCA is particularly valuable because it:</p>
        <ul>
            <li>Incorporates the decision-making context through threshold probabilities that reflect real-world scenarios</li>
            <li>Accounts for the relative costs of different types of errors (false positives vs. false negatives)</li>
            <li>Provides actionable insights about when a model should or should not be used</li>
            <li>Facilitates comparison between different models or strategies across various contexts</li>
        </ul>

        <p>For more information about DCA, visit <a href="https://mskcc-epi-bio.github.io/decisioncurveanalysis" target="_blank">https://mskcc-epi-bio.github.io/decisioncurveanalysis</a>.</p>

       </div>

       <div class="guide-section">
        <h2 id="predictive-utility-analysis">Predictive Utility Analysis</h2>
        
        <p>Predictive utility analysis consists of two key components: (1) estimating real-world predictive value and clinical utility of research findings by accounting for the outcome base rate, and (2) determining how much performance is lost due to measurement reliability.</p>
        
        <p>Predictive utility analysis can be applied in two main scenarios: interpreting existing research findings and planning new studies. Each scenario follows a different workflow, as outlined below.</p>

        <h3>1. Interpreting Existing Research Findings</h3>
        
        <p>When evaluating published research or your own completed studies, the workflow starts with observed metrics and works forward to understand real-world utility:</p>
        
        <ol>            
            <li><strong>Set measurement reliability</strong>: Ideally, reliability estimates should come directly from the same data set as the observed effect sizes. If not available, use expected estimates based on other relevant research in the field. In principle, one could perform predictive utility analysis by ignoring measurement reliability (setting all reliabilities to 1, for example). This would still allow to estimate real-world utility, but we would not know how diminished predictive performance is due to measurement reliability.</li>

            <li><strong>Set observed metrics</strong>: This can be effect size (e.g., Cohen's d, Pearson's r) or predictive performance metric (e.g., ROC-AUC, PR-AUC). It is important to ensure that you are using a robust estimate here (not inflated due to small sample or overfitting).</li>
            
            <li><strong>Set base rate</strong>: Set the base rate to reflect the expected prevalence in the real-world conditions where the predictor or model will be applied — not the composition of the study sample. For example, if the study used a balanced case-control design (50% cases, 50% controls) but the condition affects only 5% of the target population, use 5% as the base rate to accurately estimate real-world predictive utility.</li>
            
            <li><strong>Set classification threshold</strong>: Determine a classification threshold that corresponds to a realistic threshold probability (p<sub>t</sub>) in Decision Curve Analysis. This threshold should reflect the clinical or practical context where the predictor will be used, balancing the costs of false positives against the benefits of true positives.</li>
            
            <li><strong>Document relevant metrics</strong>: Note down all the relevant metrics for the chosen threshold: ROC-AUC, PR-AUC, PPV, NPV, and Net Benefit. These metrics provide a comprehensive picture of the predictor's real-world utility.</li>
        </ol>

        <h3>2. Planning Studies</h3>
        
        <p>When planning new research, the workflow starts from desired clinical utility and works backward to determine what effect sizes and study design are needed:</p>
        
        <ol>
            <li><strong>Identify clinically meaningful targets</strong>: Start by determining what level of PR-AUC, PPV, NPV, or Net Benefit would be clinically meaningful in your specific context. These targets should reflect the minimum utility needed to justify using the predictor in practice. This could come from performing cost-benefit analysis, or from existing guidelines, or from using already existing clinical instruments as a benchmark.</li>
            
            <li><strong>Determine required effect sizes</strong>: Use the simulator to determine how these clinical targets translate to the needed effect sizes (e.g., Cohen's d, ROC-AUC) by setting the base rate to the expected prevalence in the real-world setting where the predictor or model will be applied. Compare these required effect sizes with typical effect sizes found in relevant research literature to assess feasibility.</li>
            
            <li><strong>Explore ways to improve performance</strong>: Use the simulator to explore how much closer you can get to your goal by:
                <ul>
                    <li><strong>Improving measurement reliability</strong> of each predictor (e.g., using more reliable assessment methods or improving measurement protocols)</li>
                    <li><strong>Using multiple predictors</strong>: estimate how many predictors are needed to achieve a desired prediction performance. For  average effect size and average collinearity among predictors you can pick what is common in the field or what you find in your own data. Using these values in the multivariable calculator will provide you with a rough estimate of how well a multivariable model wwould perform without even needing to train it. </li>
                </ul>
            </li>
            
            <li><strong>Assess feasibility</strong>: This analysis can help determine the feasibility of both individual markers and multivariable models. </li>
        </ol>

       </div>

       <div class="guide-section">
        <h2 id="examples">Quick Start Examples</h2>
        
        To further clarify how the tool can be used and to demonstrate its utility we provide some specific examples.

        <h3>Example 1: Diagnostic Prediction: FDA-Cleared Biomarker for Alzheimer's Disease</h3>
        <p> Recently, <a href="https://www.fda.gov/news-events/press-announcements/fda-clears-first-blood-test-used-diagnosing-alzheimers-disease" target="_blank">FDA has cleared</a> plasma p-tau217/Aβ42 ratio for assessing amyloid positivity, which is a key criterion for Alzheimer's diagnosis. Let us unpack this example in E2P Simulator as an anchor for all other examples. We will use the <a href="https://www.accessdata.fda.gov/cdrh_docs/pdf24/K242706.pdf" target="_blank">report</a> that FDA clearance was based on as a reference for the parameters.
            
        </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set the base rate to 51.1%. This is amyloid positivity prevalence in the clearance cohort, which consists targets individuals with mild cognitive impairment. Note, in practice we may expect this to vary between ~ 20% to 80% depending on the average age of the cohort (<a href="https://doi.org/10.1001/jama.2015.4668" target="_blank">Jansen et al., 2015</a>).</li>
                    <li>Set the outcome reliability to 0.9, which is the reliability of PET/CSF reference agreement used to confirm amyloid positivity in the clearance cohort (<a href="https://doi.org/10.1097/RLU.0000000000001693" target="_blank">Harn et al., 2017</a>).</li>
                    <li>Set the predictor reliability for both groups to 0.9 (test-retest reliability of the assay components; <a href="https://doi.org/10.1038/s41398-024-03084-7" target="_blank">Della Monica et al., 2024</a>).</li>
                    <li>Setting the effect size in this case is a bit tricky because it is not reported. However, we can instead use the reported PPV and NPV. The report used two thresholds one for PPV and one for NPV, leaving ~20% of indeterminate cases for confirmatory PET/CSF. We only need to consider one of the thresholds to recreate the distributions: for the positive threshold they report 201/255 being true positives and 54/255 being false positives, which corresponds to PPV ≈ 0.92 and NPV ≈ 0.81. Now we adjust the effect size and classification threshld until we achieve these values.
                       </li>
                </ul>
            </li>

            <li> We find this corresponds to observed effect size d = 2.25, yielding ROC-AUC = 0.94 and PR-AUC = 0.95. We also find that the positive threshold corresponds to p<sub>t</sub> ≈ 0.68, recovering PPV = 0.92, NPV = 0.81, and resulting in ΔNB = 0.324. </li>

            <li>Measurement reliability in this case is already very high and improving it will not result in a substantial improvement in predictive performance.</li>

            <li>Even if the base rate drops to 30% (e.g., younger mild cognitive impairment cohorts), the scenario still yields PR-AUC = 0.89.</li>
        </ol>

        <button class="example-button" data-url="index.html?mode=binary&baseRate=0.511&groupingReliability=0.9&predictorReliabilityGroup1=0.9&predictorReliabilityGroup2=0.9&trueEffectSize=2.39&label1=Amyloid%20negative&label2=Amyloid%20positive&xaxisLabel=p-tau217%2FA%CE%B242&thresholdValue=1.53">Explore this example yourself&nbsp;&nbsp;&nbsp;→</button>

        <h3>Example 2: Diagnostic Prediction: Depression</h3>
        <p> Can we predict depression diagnosis using a cognitive biomarker?  </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 8% (the base rate of depression in adolescents; <a href="https://doi.org/10.1111/bjc.12333" target="_blank">Shorey et al., 2022</a>)</li>
                    <li>Set the grouping reliability to 0.28 (depression diagnosis reliability based on DSM-5 field trials; <a href="https://pubmed.ncbi.nlm.nih.gov/23111466/" target="_blank">Regier et al., 2013</a>)</li>
                    <li>Set the predictor reliability for both groups to 0.6 (an average reliability for cognitive measures; <a href="https://www.sciencedirect.com/science/article/pii/S0149763423001069" target="_blank">Karvelis et al., 2023</a>)</li>
                    <li>Set the observed effect size to d = 0.8 (a large effect size that is optimistic and rarely seen in practice)</li>
                </ul>
            </li>

            <li>With these parameters, the observed Cohen's d = 0.8 will yield ROC-AUC = 0.71 and PR-AUC = 0.19. This means that even with a "large" effect size of 0.8, the predictive utility remains rather modest, especially when it comes to the tradeoff between PPV and Sensitivity (as shown by the low PR-AUC). Using the DCA plot to set the classification threshold to correspond to 15% risk, p<sub>t</sub> = 0.15, we obtain PPV = 0.22, Sensitivity = 0.31, and ΔNB = 0.009, which means that at this threshold 78% of diagnoses would be false positives while still missing 69% of actual cases, and we would get 9 additional true positives per 1,000 diagnoses.</li>

            <li>Note that with the low reliability values, this observed effect corresponds to a much larger true effect, d = 1.58, and in turn much better predictive performance, ROC-AUC = 0.87 and PR-AUC = 0.46, and PPV = 0.27, Sensitivity = 0.74, and ΔNB = 0.046 at p<sub>t</sub> = 0.1, highlighting how much improvement in diagnostic prediction could be achieved simply by improving measurement reliability.</li>

            <li>Now let's say we are serious about precision psychiatry and we want to achieve a PR-AUC of 0.8. Using the tool, we can find that it would require ROC-AUC = 0.96. It would be rather unrealistic to expect a single biomarker to achieve this effect size. Using the multivariable simulator, you can explore how many predictors with smaller d values would be required to achieve a desired prediction performance.</li>

        </ol>
        
        <button class="example-button" data-url="index.html?mode=binary&baseRate=0.08&groupingReliability=0.28&predictorReliabilityGroup1=0.6&predictorReliabilityGroup2=0.6&trueEffectSize=1.58&label1=Controls&label2=Depression&xaxisLabel=Cognitive%20marker&thresholdValue=1.67">Explore this example yourself&nbsp;&nbsp;&nbsp;→</button>
        

        <h3>Example 3: Treatment Response Prediction: Antidepressants</h3>
        <p> Can we predict who will respond to antidepressant treatment using task-based brain activity measures?</p>
        <ol>
            <li>Select Continuous outcome mode:</li>
                <ul>
                    <li>Set base rate to 15% (the rate of response to antidepressant treatment beyond placebo; <a href="https://www.bmj.com/content/378/bmj-2021-067606" target="_blank">Stone et al., 2022</a>)</li>
                    <li>Set predictor reliability to 0.4 (an average reliability for task-fMRI measures; <a href="https://journals.sagepub.com/doi/full/10.1177/0956797620916786" target="_blank">Elliott et al., 2020</a>)</li>
                    <li>Set outcome reliability to 0.94 (Hamilton Depression Rating Scale (HAMD) reliability; <a href="https://pubmed.ncbi.nlm.nih.gov/21276619/" target="_blank">Trajković et al., 2011</a>)</li>
                    <li>Adjust effect size such that R² = 0.2 (average multivariate R² from recent research; <a href="https://doi.org/10.1162/netn_a_00233" target="_blank">Karvelis et al., 2022</a>)</li>
                </ul>

            <li>This will yield ROC-AUC = 0.73 and PR-AUC = 0.33, indicating rather modest predictive performance, as shown by the low PR-AUC. At p<sub>t</sub> = 0.2, which reflects the relative harms of antidepressant side effects, this would result in Sensitivity = 0.54, PPV = 0.29, and ΔNB = 0.031, which means that 46% of those who would benefit from treatment would not receive treatment, 71% of those given treatment would not benefit from it, and we would get additional 3.1 true responders per 100 people who receive antidepressants. Improving measurement reliability alone could improve performance quite substantially, up to ROC-AUC = 0.87 and PR-AUC = 0.57, which at p<sub>t</sub> = 0.2 would give Sensitivity = 0.74, PPV = 0.42, and ΔNB = 0.073.</li>

            <li>If we want to once again be serious about precision psychiatry and aim for PR-AUC of 0.8, we will find it requires R² = 0.8. These are rather extremely ambitious values (requiring to explain 80% of variance in symptom improvement). This helps demonstrate the inherent limitations of dichotomizing continuous outcomes for evaluating treatment response prediction - doing so leads to a loss of valuable information. On the other hand, it does reflect the binary nature of decision-making in psychiatry (to prescribe the treatment or not).</li>
        </ol>

        <button class="example-button" data-url="index.html?mode=continuous&predictorReliability=0.4&outcomeReliability=0.94&effectSizeR=0.73&baseRate=0.15&label1=Non-responders&label2=Responders&xaxisLabel=Task-fMRI%20marker&yaxisScatterLabel=Treatment%20response&thresholdValue=0.6">Explore this example yourself&nbsp;&nbsp;&nbsp;→</button>

        <h3>Example 4: Risk Prediction: Suicide Attempts</h3>
        <p> Can we predict who will attempt suicide using electronic health records?</p> 
            
        <p>
            One of the largest prospective suicide prediction studies (<a href="https://doi.org/10.1097/MLR.0000000000001445" target="_blank">Edgcomb et al., 2021</a>) followed women (N = 67,000) with serious mental illness for 12 months after a general medical hospitalization and trained models on pre-discharge electronic health records to predict readmission for suicide attempt or self-harm, achieving ROC-AUC of 0.73 (derivation sample) and 0.71 (external sample). A companion study (<a href="https://doi.org/10.1016/j.jpsychires.2022.10.035" target="_blank">Thiruvalluru et al., 2023</a>) in men (N = 1.4 million) reported a 12-month base rate of 3.9% and similar discrimination.
        </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 3.9% (<a href="https://doi.org/10.1016/j.jpsychires.2022.10.035" target="_blank">Thiruvalluru et al., 2023</a>)</li>
                    <li>Set the outcome reliability to 1.0 (hospital admissions for attempts or self-harm can be assumed to be near perfect)</li>
                    <li>Set the predictor reliability for both groups to 0.8 (structured electronic health records including healthcare utilization, prior attempts, psychiatric diagnoses, etc., can be assumed to have rather high reliability)</li>
                    <li>Set the observed effect size to d = 0.77, which corresponds to ROC-AUC = 0.71</li>
                </ul>
            </li>

            <li>This yields PR-AUC = 0.10, indicating poor predictive performance in the real world. At p<sub>t</sub> = 0.03 (a reasonable threshold for intervention), we get Sensitivity = 0.77, PPV = 0.06, and ΔNB = 0.006. This means that while the model would capture about three-quarters of true cases, only 6% of those flagged would actually attempt suicide, and the added benefit would translate to 6 additional true cases per 1,000 individuals.</li>

            <li>To achieve PR-AUC = 0.8 in this population would require ROC-AUC = 0.98, which is extremely unrealistic. At p<sub>t</sub> = 0.03, this would result in Sensitivity = 0.94, PPV = 0.30, and ΔNB = 0.025.</li>

            <li>Note that because the reliability is already quite high, improving it further would not make much of a difference. What we need to do is to find better predictors. Alternatively, it may be more effective to simply focus on universal suicide prevention strategies rather than trying to predict individual cases (e.g.,<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6296389/" target="_blank">Large 2018</a>).</li>
        </ol>

        <button class="example-button" data-url="index.html?mode=binary&baseRate=0.039&groupingReliability=1.0&predictorReliabilityGroup1=0.8&predictorReliabilityGroup2=0.8&trueEffectSize=0.86&label1=Psychiatric%20controls&label2=Suicide%20attempters&xaxisLabel=Electronic%20health%20records&thresholdValue=0.05">Explore this example yourself&nbsp;&nbsp;&nbsp;→</button>

        <h3>Example 5: Risk Prediction: Transition to Psychosis</h3>
        <p> How promising is MMN for predicting transition to psychosis within clinical high-risk (CHR) cohorts?</p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set the base rate to 19% (average 24-month conversion rate in CHR cohorts).</li>
                    <li>Set the outcome (grouping) reliability to 0.46 (DSM-5 psychosis spectrum κ).</li>
                    <li>Set the predictor reliability for both groups to 0.5 (double-deviant MMN test-retest reliability).</li>
                    <li>Set the observed effect size to d = 0.43, yielding ROC-AUC = 0.62 and PR-AUC = 0.27.</li>
                </ul>
            </li>

            <li>At p<sub>t</sub> = 0.15 (low-cost monitoring), the simulator produces PPV = 0.22, Sensitivity = 0.81, and ΔNB = 0.010, matching the incremental utility reported in the longitudinal study.</li>

            <li>Accounting for attenuation shows the underlying true effect is d ≈ 0.75, highlighting how modest discrimination partly reflects limited measurement fidelity.</li>

            <li>Improving measurement reliability (e.g., ICC ≈ 0.8) boosts performance to ROC-AUC = 0.70, PR-AUC = 0.36, PPV = 0.26, Sensitivity = 0.78, and ΔNB = 0.026 at the same threshold.</li>
        </ol>

        <button class="example-button" data-url="index.html?mode=binary&baseRate=0.19&groupingReliability=0.46&predictorReliabilityGroup1=0.5&predictorReliabilityGroup2=0.5&trueEffectSize=0.75&label1=Non-converters&label2=Converters&xaxisLabel=Double-deviant%20MMN&thresholdValue=0.15">Explore this example yourself&nbsp;&nbsp;&nbsp;→</button>
   
    </div>

    <div class="guide-section" id="sample-size">
        <h2>Sample Size Calculations for Prediction Models</h2>
        
        <p>E2P Simulator includes a sample size calculator to help you determine how much data is needed for your multivariable prediction models. This calculator is designed to help you avoid overfitting and keep prediction error low (following the recommendations from <a href="https://doi.org/10.1136/bmj.m441" target="_blank">Riley et al., 2020</a>), providing more robust sample size estimates than simple rules of thumb like "10 events per predictor".</p>

        <h3>How to Use</h3>
        <p>Specify your number of predictors, <strong>realistically expected R²</strong> (based on prior research or pilot data), and outcome base rate (for binary outcomes only). Use R²<sub>CS</sub> for binary outcomes and standard R² for continuous outcomes. Note, for binary outcomes with a single continuous predictor, R²<sub>CS</sub> equals eta-squared (η²), which is already displayed in the main E2P simulator dashboard. The final recommendation uses the <em>maximum</em> across all criteria to ensure all performance targets are met.

        <p>The sample size calculators complement the main E2P simulators in study planning: the E2P simulators explore relationships between effect sizes and predictive utility (both what you need for desired performance and what utility to expect from realistic effects), while the sample size calculators determine adequate sample size based on realistic R² estimates from prior research. For sample size planning, always use conservative, realistic R² estimates based on prior research, not idealized target values.</p>

        <h3>Prediction Models vs. Hypothesis Testing Sample Sizes</h3>
        <p>You may wonder how these prediction-focused sample size calculations compare to traditional power analysis used in hypothesis testing. The key difference is that power analysis focuses on detecting whether an effect exists, while prediction-focused calculations prioritize model reliability and performance on new data. This fundamental difference in goals typically leads to larger sample size requirements for prediction models.</p>
        
        <p>Another way to think about this difference is in terms of precision requirements. Power analysis only needs sufficient precision to distinguish an effect from zero (statistical significance). In contrast, prediction models require much tighter confidence intervals around parameter estimates to ensure much more precise estimation of predictive performance / effect sizes.</p>

    </div>

    <div class="guide-section">
        <h2 id="feedback">Feedback and Contributions</h2>
        <p>E2P Simulator is an open-source project - feedback, bug reports, and suggestions for improvement are welcome. The easiest way to do so is through the <a href="https://github.com/povilaskarvelis/e2p-simulator/issues" target="_blank">GitHub Issues page</a>.</p>
        <p>You can view the source code, track development, and contribute directly at the <a href="https://github.com/povilaskarvelis/e2p-simulator" target="_blank">project's GitHub repository</a>.</p>
        <p>For other inquiries, you can find my contact information <a href="https://povilaskarvelis.github.io/" target="_blank">here</a>.</p>
    </div>

    <div class="guide-section">
        <h2 id="references">References</h2>
        <ol style="font-size: 0.95em; line-height: 1.8;">
            <li>Brabec, J., Komárek, T., Franc, V., & Machlica, L. (2020). On model evaluation under non-constant class imbalance. <em>International Conference on Computational Science</em>, vol. 12140 (pp. 74-87). Springer, Cham. <a href="https://doi.org/10.1007/978-3-030-50423-6_6" target="_blank">https://doi.org/10.1007/978-3-030-50423-6_6</a></li>
            
            <li>Christodoulou, E., Ma, J., Collins, G. S., Steyerberg, E. W., Verbakel, J. Y., & Van Calster, B. (2019). A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. <em>Journal of Clinical Epidemiology, 110</em>, 12-22. <a href="https://doi.org/10.1016/j.jclinepi.2019.02.004" target="_blank">https://doi.org/10.1016/j.jclinepi.2019.02.004</a></li>

            <li>Della Monica, C., Revell, V., Atzori, G., Laban, R., Skene, S. S., Heslegrave, A., ... & Dijk, D.-J. (2024). P-tau217 and other blood biomarkers of dementia: Variation with time of day. <em>Translational Psychiatry, 14</em>(1), 373. <a href="https://doi.org/10.1038/s41398-024-03084-7" target="_blank">https://doi.org/10.1038/s41398-024-03084-7</a></li>

            <li>Edgcomb, J. B., Thiruvalluru, R., Pathak, J., Brooks, J. O., & Zima, B. (2021). Machine learning to differentiate risk of suicide attempt and self-harm after general medical hospitalization of women with mental illness. <em>Medical Care, 59</em>, S58-S64. <a href="https://journals.lww.com/lww-medicalcare/fulltext/2021/02001/machine_learning_to_differentiate_risk_of_suicide.14.aspx" target="_blank">https://doi.org/10.1097/MLR.0000000000001445</a></li>
            
            <li>Elliott, M. L., Knodt, A. R., Ireland, D., Morris, M. L., Poulton, R., Ramrakha, S., Sison, M. L., Moffitt, T. E., Caspi, A., & Hariri, A. R. (2020). What is the test-retest reliability of common task-functional MRI measures? New empirical evidence and a meta-analysis. <em>Psychological Science, 31</em>(7), 792-806. <a href="https://journals.sagepub.com/doi/full/10.1177/0956797620916786" target="_blank">https://doi.org/10.1177/0956797620916786</a></li>
            
            <li>Food and Drug Administration. (2025). FDA Clears First Blood Test Used in Diagnosing Alzheimer's Disease. <em>FDA News Release</em>. <a href="https://www.fda.gov/news-events/press-announcements/fda-clears-first-blood-test-used-diagnosing-alzheimers-disease" target="_blank">https://www.fda.gov/news-events/press-announcements/fda-clears-first-blood-test-used-diagnosing-alzheimers-disease</a></li>
            
            <li>Food and Drug Administration. (2025). 510(k) Premarket Notification: Lumipulse G p-Tau 217/β-Amyloid 1-42 Plasma Ratio. <em>FDA 510(k) Summary, K242706</em>. <a href="https://www.accessdata.fda.gov/cdrh_docs/pdf24/K242706.pdf" target="_blank">https://www.accessdata.fda.gov/cdrh_docs/pdf24/K242706.pdf</a></li>
            
            <li>Harn, N. R., Hunt, S. L., Hill, J., Vidoni, E., Perry, M., & Burns, J. M. (2017). Augmenting amyloid PET interpretations with quantitative information improves consistency of early amyloid detection. <em>Clinical Nuclear Medicine, 42</em>(8), 577-581. <a href="https://doi.org/10.1097/RLU.0000000000001693" target="_blank">https://doi.org/10.1097/RLU.0000000000001693</a></li>
            
            <li>Jansen, W. J., Ossenkoppele, R., Knol, D. L., Tijms, B. M., Scheltens, P., Verhey, F. R., ... & Amyloid Biomarker Study Group. (2015). Prevalence of cerebral amyloid pathology in persons without dementia: A meta-analysis. <em>JAMA, 313</em>(19), 1924-1938. <a href="https://doi.org/10.1001/jama.2015.4668" target="_blank">https://doi.org/10.1001/jama.2015.4668</a></li>
            
            <li>Karvelis, P., & Diaconescu, A. O. (2025). Clarifying the reliability paradox: poor measurement reliability attenuates group differences. <em>Frontiers in Psychology, 16</em>, 1592658. <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1592658" target="_blank">https://doi.org/10.3389/fpsyg.2025.1592658</a></li>
            
            <li>Karvelis, P., & Diaconescu, A. O. (2025). E2P Simulator: An Interactive Tool for Estimating Real-World Predictive Utility of Research Findings. <em>Journal of Open Source Software, 10</em>(114), 8334. <a href="https://doi.org/10.21105/joss.08334" target="_blank">https://doi.org/10.21105/joss.08334</a></li>
            
            <li>Karvelis, P., Paulus, M. P., & Diaconescu, A. O. (2023). Individual differences in computational psychiatry: A review of current challenges. <em>Neuroscience & Biobehavioral Reviews, 148</em>, 105137. <a href="https://doi.org/10.1016/j.neubiorev.2023.105137" target="_blank">https://doi.org/10.1016/j.neubiorev.2023.105137</a></li>
            
            <li>Karvelis, P., Charlton, C. E., Allohverdi, S. G., Bedford, P., Hauke, D. J., & Diaconescu, A. O. (2022). Computational approaches to treatment response prediction in major depression using brain activity and behavioral data: A systematic review. <em>Network Neuroscience, 6</em>(4), 1066-1103. <a href="https://doi.org/10.1162/netn_a_00233" target="_blank">https://doi.org/10.1162/netn_a_00233</a></li>
            
            <li>Large, M. M. (2018). The role of prediction in suicide prevention. <em>Dialogues in Clinical Neuroscience, 20</em>(3), 197-205. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6296389/" target="_blank">https://doi.org/10.31887/DCNS.2018.20.3/mlarge</a></li>
            
            <li>Regier, D. A., Narrow, W. E., Clarke, D. E., Kraemer, H. C., Kuramoto, S. J., Kuhl, E. A., & Kupfer, D. J. (2013). DSM-5 field trials in the United States and Canada, Part II: Test-retest reliability of selected categorical diagnoses. <em>American Journal of Psychiatry, 170</em>(1), 59-70. <a href="https://pubmed.ncbi.nlm.nih.gov/23111466/" target="_blank">https://doi.org/10.1176/appi.ajp.2012.12070999</a></li>
            
            <li>Riley, R. D., Ensor, J., Snell, K. I. E., Harrell Jr, F. E., Martin, G. P., Reitsma, J. B., Moons, K. G. M., Collins, G., & van Smeden, M. (2020). Calculating the sample size required for developing a clinical prediction model. <em>BMJ, 368</em>, m441. <a href="https://doi.org/10.1136/bmj.m441" target="_blank">https://doi.org/10.1136/bmj.m441</a></li>
            
            <li>Shorey, S., Ng, E. D., & Wong, C. H. J. (2022). Global prevalence of depression and elevated depressive symptoms among adolescents: A systematic review and meta-analysis. <em>British Journal of Clinical Psychology, 61</em>(2), 287-305. <a href="https://doi.org/10.1111/bjc.12333" target="_blank">https://doi.org/10.1111/bjc.12333</a></li>
            
            <li>Stone, M. B., Yaseen, Z. S., Miller, B. J., Richardville, K., Kalaria, S. N., & Kirsch, I. (2022). Response to acute monotherapy for major depressive disorder in randomized, placebo controlled trials submitted to the US Food and Drug Administration: Individual participant data analysis. <em>BMJ, 378</em>, e067606. <a href="https://www.bmj.com/content/378/bmj-2021-067606" target="_blank">https://doi.org/10.1136/bmj-2021-067606</a></li>
            
            <li>Thiruvalluru, R. K., Edgcomb, J. B., Brooks, J. O., & Pathak, J. (2023). Risk of suicide attempts and self-harm after 1.4 million general medical hospitalizations of men with mental illness. <em>Journal of Psychiatric Research, 157</em>, 50-56. <a href="https://doi.org/10.1016/j.jpsychires.2022.10.035" target="_blank">https://doi.org/10.1016/j.jpsychires.2022.10.035</a></li>
            
            <li>Trajković, G., Starčević, V., Latas, M., Leštarević, M., Ille, T., Bukumirić, Z., & Marinković, J. (2011). Reliability of the Hamilton Rating Scale for Depression: A meta-analysis over a period of 49 years. <em>Psychiatry Research, 189</em>(1), 1-9. <a href="https://pubmed.ncbi.nlm.nih.gov/21276619/" target="_blank">https://doi.org/10.1016/j.psychres.2010.12.007</a></li>
            
            <li>Vickers, A. J., & Elkin, E. B. (2006). Decision curve analysis: A novel method for evaluating prediction models. <em>Medical Decision Making, 26</em>(6), 565-574. <a href="https://pubmed.ncbi.nlm.nih.gov/17099194/" target="_blank">https://doi.org/10.1177/0272989X06295361</a></li>
        </ol>
    </div>

    <!-- Footer -->
    <footer style="text-align: center; padding: 20px; margin-top: 40px; border-top: 1px solid #eee;">
        <p style="color: #666; font-size: 0.9em;">
            E2P Simulator is open source software. 
            <a href="https://github.com/povilaskarvelis/e2p-simulator" style="color: #0366d6; text-decoration: none;">View on GitHub</a> | 
            <a href="https://github.com/povilaskarvelis/e2p-simulator/blob/main/LICENSE" style="color: #0366d6; text-decoration: none;">MIT License</a>
        </p>
        <p style="color: #666; font-size: 0.9em; margin-top: 10px;">
            Developed by <a href="https://povilaskarvelis.github.io/" style="color: #0366d6; text-decoration: none;" target="_blank">Povilas Karvelis</a>
        </p>
    </footer>

    <!-- Load minimal scripts for get-started page -->
    <script>
        // Minimal initialization for get-started page
        document.addEventListener('DOMContentLoaded', function() {
            // Only fetch version info, no binary/continuous initialization needed
            fetchVersionInfo();
            
            // Initialize TOC active section tracking
            initTocTracking();
            
            // Initialize example buttons
            document.querySelectorAll('.example-button').forEach(btn => {
                btn.addEventListener('click', () => {
                    window.location.href = btn.dataset.url;
                });
            });
        });
        
        // TOC active section tracking
        function initTocTracking() {
            const tocLinks = document.querySelectorAll('.toc-sidebar a');
            const tocSidebar = document.querySelector('.toc-sidebar');
            const footer = document.querySelector('footer');
            const firstSection = document.querySelector('.guide-section');
            const sections = [];
            
            // Get the initial position aligned with first section
            const firstSectionTop = firstSection ? firstSection.offsetTop : 200;
            const stickyTop = 80; // Fixed top position when sticky
            
            // Set initial top position
            tocSidebar.style.top = firstSectionTop + 'px';
            
            // Gather all section elements
            tocLinks.forEach(link => {
                const id = link.getAttribute('href').substring(1);
                const section = document.getElementById(id);
                if (section) {
                    sections.push({ id, element: section, link });
                }
            });
            
            // Update active section on scroll
            function updateActiveSection() {
                const scrollPosition = window.scrollY + 120; // Offset for header
                const documentHeight = document.documentElement.scrollHeight - window.innerHeight;
                const scrollProgress = (window.scrollY / documentHeight) * 100;
                
                // Update progress indicator
                tocSidebar.style.setProperty('--scroll-progress', scrollProgress + '%');
                
                // Adjust top position: follows content initially, then sticks at 80px
                const scrollY = window.scrollY;
                const dynamicTop = firstSectionTop - scrollY;
                
                if (dynamicTop > stickyTop) {
                    // Still above sticky threshold - follow the content
                    tocSidebar.style.top = dynamicTop + 'px';
                } else {
                    // At or past sticky threshold - stick at 80px
                    tocSidebar.style.top = stickyTop + 'px';
                }
                
                // Hide sidebar when it would overlap with footer
                if (footer) {
                    const footerRect = footer.getBoundingClientRect();
                    const sidebarRect = tocSidebar.getBoundingClientRect();
                    const buffer = 20; // Extra buffer space
                    
                    if (footerRect.top < sidebarRect.bottom + buffer) {
                        tocSidebar.style.opacity = '0';
                        tocSidebar.style.pointerEvents = 'none';
                    } else {
                        tocSidebar.style.opacity = '1';
                        tocSidebar.style.pointerEvents = 'auto';
                    }
                }
                
                // Find active section
                let activeSection = sections[0];
                
                for (const section of sections) {
                    if (section.element.offsetTop <= scrollPosition) {
                        activeSection = section;
                    }
                }
                
                // Update active class
                tocLinks.forEach(link => link.classList.remove('active'));
                if (activeSection) {
                    activeSection.link.classList.add('active');
                }
            }
            
            // Throttle scroll events for performance
            let ticking = false;
            window.addEventListener('scroll', function() {
                if (!ticking) {
                    window.requestAnimationFrame(function() {
                        updateActiveSection();
                        ticking = false;
                    });
                    ticking = true;
                }
            });
            
            // Initial update
            updateActiveSection();
        }
        
        // Copy the version fetch function from main.js
        async function fetchVersionInfo() {
            try {
                const response = await fetch('https://api.github.com/repos/povilaskarvelis/e2p-simulator/releases/latest');
                if (response.ok) {
                    const data = await response.json();
                    const versionNumber = data.tag_name || data.name;
                    
                    // Update the version display
                    const versionElement = document.getElementById('version-number');
                    if (versionElement && versionNumber) {
                        versionElement.textContent = versionNumber;
                        versionElement.style.color = '#0366d6'; // GitHub blue color
                    }
                } else {
                    // If API call fails, hide the version info or show fallback
                    const versionInfo = document.getElementById('version-info');
                    if (versionInfo) {
                        versionInfo.style.display = 'none';
                    }
                }
            } catch (error) {
                console.log('Could not fetch version info:', error);
                // Hide version info if fetch fails
                const versionInfo = document.getElementById('version-info');
                if (versionInfo) {
                    versionInfo.style.display = 'none';
                }
            }
        }
    </script>
</body>
</html> 