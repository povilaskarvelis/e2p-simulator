<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="Getting started with the Effect Size Calculator - A guide for understanding and using the tool">
    <meta name="viewport" content="width=1400, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Get Started - E2P Simulator</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VDYP5LLPT5"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VDYP5LLPT5');
    </script>
    
    <!-- Load libraries first -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.1/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <!-- Load styles -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/binary.css">
    <link rel="stylesheet" href="css/continuous.css">
    
    <!-- MathJax for professional equation rendering -->
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.auto.min.js"></script>
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
        },
        svg: {
            scale: 1,
            minScale: .5,
            mtextInheritFont: false,
            merrorInheritFont: true,
            mathmlSpacing: false,
            skipAttributes: {},
            exFactor: .5,
            displayAlign: 'center',
            displayIndent: '0',
            versionWarning: false,
        },
        options: {
            enableMenu: false,
            menuOptions: {
                settings: {
                    zoom: "Click",
                    zscale: "200%",
                }
            }
        }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Header -->
    <header class="site-header">
        <a href="index.html" class="site-title">E2P Simulator</a>
        <nav class="site-nav">
            <a href="index.html" class="nav-link">Home</a>
            <a href="get-started.html" class="nav-link active">Get Started</a>
        </nav>
    </header>

    <h1>Getting Started with E2P Simulator</h1>

    <div class="description">
        <p>Welcome to the E2P Simulator! This guide will help you understand how to use this tool to explore 
           the relationship between effect sizes and their predictive utility.</p>
    </div>

    <div class="guide-section">
        <h2>What is the E2P Simulator?</h2>
        <p>The E2P Simulator (Effect-to-Prediction Simulator) bridges the gap between statistical effect sizes and predictive utility. It interactively and visually shows how commonly reported effect size metrics (like Cohen's d and Pearson's r) translate into predictive performance, while accounting for measurement reliability and outcome base rates.</p>

        <p>The E2P Simulator has several potential applications:</p>
        
        <ul>
            <li><strong>Interpretation of findings</strong>: It helps researchers move beyond arbitrary "small/medium/large" effect size labels, and instead ground their interpretations in terms of predictive value within specific contexts.</li>
            <li><strong>Research planning</strong>: Being able to easily derive what effect sizes are needed to achieve the desired predictive performance allows researchers to plan their studies more effectively and allocate resources more efficiently.</li>
            <li><strong>Education</strong>: The simulator's interactive design makes it a valuable teaching tool, helping students and researchers develop a more intuitive understanding of how different abstract statistical metrics relate to each other and to real-world impact.</li>
        </ul>
        
    </div>

    <div class="guide-section">
        <h2>Key Features</h2>

        <h3>Binary vs. Continuous Outcomes</h3>
        
        <p>The E2P Simulator offers two complementary analysis modes: when outcomes are binary and when outcomes are continuous (in machine learning language, that corresponds to classification and prediction tasks, respectively).</p>
        
        <ul>
            <li><strong>Binary</strong>: For exploring how effect size measures of binary outcomes (e.g., cases vs. controls) translate to predictive metrics.</li>
            
            <li><strong>Continuous</strong>: For exploring how continuous outcomes (e.g., symptom/performance improvement) that are later thresholded for practical purposes (e.g., responder vs. non-responder) translate to predictive metrics.</li>
        </ul>
        
        <h3>Measurement Reliability and True vs. Observed Effects</h3>
        <p>All measurements contain some degree of error, which attenuates the effect sizes we observe in practice. The simulator allows you to explore this relationship by adjusting the reliability of the measurements and toggling between true effect sizes (what would be observed with perfect measurement) and observed effect sizes (what you actually see given a specific level of reliability).</p>
            
        <p>Note that the simulator does not account for sample size limitations, which can introduce additional uncertainty around the true effect size through sampling error.</p>
        
        <h3>Base Rates</h3>
        <p>The base rate (prevalence) of outcomes dramatically affects predictive utility. Even with strong effect sizes, rare outcomes (e.g., suicide attempts) would be very difficult to predict with high precision. The simulator demonstrates how the same effect size can yield very different predictive performance depending on base rates.</p>
            
        <p>Note that base rate considerations here are not exactly the same as having imbalanced classes in machine learning, which refers to unequal group sizes in collected data. Instead, here we want to consider how predictive performance found using balanced groups would translate to real-world prevalence.</p>

    </div>

    <div class="guide-section">
        <h2 id="understanding-metrics">Understanding Predictive Metrics</h2>
        
        <h3>Classification Outcomes and Metrics</h3>
        <p>When using a predictor to classify cases into two groups, there are four possible outcomes: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). These form the basis for all predictive metrics.</p>
        
        <div id="predictive-metrics-diagram" style="text-align: center; margin: 20px 0;">
            <img src="images/prediction_metrics.png" alt="Prediction metrics diagram showing confusion matrix and all derived metrics" class="screenshot" style="max-width: 90%;">
        </div>
        
        <p>The image above illustrates how these four outcomes relate to various classification metrics. On the left, you can see how negative (e.g., controls) and positive (e.g., cases) distributions overlap and how a classification threshold (red line) creates these four outcomes. On the right, you'll find the confusion matrix and the formulas for key metrics derived from it. Note how some metrics have multiple names (e.g., Sensitivity/Recall/TPR, Precision/PPV) - this reflects how the same concepts are referred to differently across fields like medicine, cognitive science, and machine learning.</p>
        
        <h3>Key Metrics for Different Contexts</h3>
        <p>Depending on your research context, certain metrics may be more relevant than others:</p>
        <ul>
            <li><strong>When false negatives are costly</strong> (e.g., missing a disease diagnosis): Focus on sensitivity/recall</li>
            <li><strong>When false positives are costly</strong> (e.g., unnecessary treatments): Focus on specificity</li>
            <li><strong>When you need to know how much to trust a positive result</strong>: Focus on precision/PPV</li>
            <li><strong>When working with rare outcomes</strong>: Consider balanced accuracy, F1 score, or MCC</li>
        </ul>
        
        <h3>Threshold-Independent Metrics</h3>
        <p>Some metrics evaluate performance across all possible thresholds:</p>
        <ul>
            <li><strong>AUC (Area Under the ROC Curve)</strong>: Visualizes how well a model balances true positives (sensitivity) and false positives (1-specificity) across all possible thresholds, capturing the trade-off between the two. An AUC of 0.5 means the model is no better than flipping a coin, while 1.0 means perfect separation between groups.</li>
            <li><strong>PR-AUC (Area Under the Precision-Recall Curve)</strong>: Shows how well a model can maintain both precision (PPV) and recall (sensitivity) together, which is particularly informative in the context of rare outcomes, as the base rate affects precision. A larger PR-AUC means the model can achieve high precision without sacrificing recall (or vice versa), indicating a smaller trade-off between finding all positive cases and avoiding false alarms.</li>
        </ul>
       </div>

    <div class="guide-section">

        <h2>Understanding Multivariate Effects with Calculators</h2>
        <p>Both binary and continuous outcomes analysis modes include calculators that help explore how multiple predictors can be combined to achieve stronger effects:</p>
        
        <ul>
            <li><strong>Mahalanobis D Calculator</strong> (Binary mode)</li>
            <li><strong>Multivariate R² Calculator</strong> (Continuous mode)</li>
        </ul>
        
        <p>The calculators can help approximate the expected performance of multivariate models without having to train the full models and thus help with research planning and model development. More specifically, they illustrate:</p>
        
        <ul>
            <li>How the number of predictors affects combined effect size</li>
            <li>The diminishing returns of adding more predictors</li>
            <li>How collinearity (correlation among predictors) reduces their combined effectiveness</li>
            <li>The trade-offs between using fewer strong predictors versus more moderate ones</li>
        </ul>     

        <h3>Calculator Assumptions and Limitations</h3>
        <p>Both calculators correspond to fundamental predictive models in statistics: the Mahalanobis D Calculator approximates Linear Discriminant Analysis and logistic regression, while the Multivariate R² Calculator aligns with multiple linear regression.</p>
        
        <p>Like the statistical methods they approximate, the calculators operate under several simplifying assumptions:</p>
        <ul>
            <li><strong>Average effects and correlations</strong>: The calculators use single values to represent the average effect size across predictors and average correlation among them, which can provide useful approximations even when individual predictors vary in strength</li>
            <li><strong>Linear effects</strong>: The formulas assume predictors contribute additively without interactions (where one predictor's effect depends on another). However, research shows that in clinical prediction, complex non-linear models generally do not outperform simple linear logistic regression (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0895435618310813" target="_blank">Christodoulou et al., 2019</a>)</li>
            <li><strong>Normality</strong>: Variables are assumed to be normally distributed</li>
        </ul>
        
        <p>Despite these limitations, these calculators serve as valuable tools for building intuition about how multiple predictors combine to achieve stronger effects. Even though real-world predictors will vary in their individual strengths and collinearity, the overall patterns demonstrated (such as diminishing returns and the impact of shared variance) remain informative for understanding multivariate relationships.</p>
    </div>

    <div class="guide-section">
        <h2>Quick Start Examples</h2>
        
        <h3>Example 1: Predicting Depression Diagnosis</h3>
        <p> Can we predict depression diagnosis using a cognitive biomarker?  </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 8% (the prevalence of depression in the population; <a href="https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bjc.12333" target="_blank">Shorey et al., 2021</a>)</li>
                    <li>Set the grouping reliability to 0.28 (depression diagnosis reliability based on DSM-5 field trials; <a href="https://pubmed.ncbi.nlm.nih.gov/23111466/" target="_blank">Regier et al., 2013</a>)</li>
                    <li>Set the predictor reliability for both groups to 0.6 (an average reliability for cognitive measures; <a href="https://www.sciencedirect.com/science/article/pii/S0149763423001069" target="_blank">Karvelis et al., 2023</a>)</li>
                    <li>Set the observed effect size to d = 0.8 (a large effect size that is optimistic and rarely seen in practice)</li>
                </ul>
            </li>

            <li>This will yield AUC = 0.71 and PR-AUC = 0.19. So, even with the optimistic effect size of 0.8, the predictive utility remains very modest, especially when it comes to the tradeoff between recall and precision (as shown by the low PR-AUC).</li>

            <li>Note that with the low reliability values, this observed effect corresponds to a much larger true effect, d = 1.58, which, while not being very realistic in practice, highlights how much information is lost due to lack of measurement reliability.</li>

            <li>Now let's say we are serious about precision psychiatry and we want to achieve a PR-AUC of 0.8. Using the tool, we can find that it would require d = 2.55. It would be totally unrealistic to expect a single biomarker to achieve this effect size. Using the Mahalanobis D calculator, you can explore how many predictors with smaller d values would be required to achieve D = 2.55. This helps to plan research more effectively and evaluate progress.</li>

        </ol>
        
        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=binary&baseRate=0.08&groupingReliability=0.28&predictorReliabilityGroup1=0.6&predictorReliabilityGroup2=0.6&trueEffectSize=1.58'">Explore this example yourself →</button>
        

        <h3>Example 2: Predicting Antidepressant Response</h3>
        <p> Can we predict who will respond to antidepressant treatment using a cognitive biomarker?</p>
        <ol>
            <li>Select Continuous outcome mode:</li>
                <ul>
                    <li>Set base rate to 15% (the rate of response to antidepressant treatment beyond placebo; <a href="https://www.bmj.com/content/378/bmj-2021-067606" target="_blank">Stone et al., 2022</a>)</li>
                    <li>Set predictor reliability to 0.6 (an average reliability for cognitive measures; <a href="https://www.sciencedirect.com/science/article/pii/S0149763423001069" target="_blank">Karvelis et al., 2023</a>)</li>
                    <li>Set outcome reliability to 0.94 (Hamilton Depression Rating Scale (HAMD) reliability; <a href="https://pubmed.ncbi.nlm.nih.gov/21276619/" target="_blank">Trajković et al., 2011</a>)</li>
                    <li>Adjust effect size such that R² = 0.2 (average multivariate R² from recent research; <a href="https://direct.mit.edu/netn/article/6/4/1066/109196/Computational-approaches-to-treatment-response" target="_blank">Karvelis et al., 2022</a>)</li>
                </ul>

            <li>This will yield AUC = 0.73 and PR-AUC = 0.32, indicating rather modest predictive performance, as shown by the low PR-AUC. Note that the limiting factor in this scenario is not so much the reliability but the effect size.</li>

            <li>If we want to once again be serious about precision psychiatry and aim for PR-AUC of 0.8, we will find it requires r = 0.9 or (R² = 0.81). These are rather extremely ambitious values (requiring to explain 81% of variance in symptom improvement). This helps demonstrate the inherent limitations of dichotomizing continuous outcomes for assessing treatment response prediction - it leads to a loss of valuable information and can misrepresent the actual predictive power of the model.</li>
        </ol>

        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=continuous&predictorReliability=0.6&outcomeReliability=0.94&effectSizeR=0.6&baseRate=0.15'">Explore this example yourself →</button>
   
        
    <!-- Footer -->
    <footer style="text-align: center; padding: 20px; margin-top: 40px; border-top: 1px solid #eee;">
        <p style="color: #666; font-size: 0.9em;">
            E2P Simulator is open source software. 
            <a href="https://github.com/povilaskarvelis/e2p-simulator" style="color: #0366d6; text-decoration: none;">View on GitHub</a> | 
            <a href="https://github.com/povilaskarvelis/e2p-simulator/blob/main/LICENSE" style="color: #0366d6; text-decoration: none;">MIT License</a>
        </p>
        <p style="color: #666; font-size: 0.9em; margin-top: 10px;">
            Created by <a href="https://povilaskarvelis.github.io/" style="color: #0366d6; text-decoration: none;" target="_blank">Povilas Karvelis</a>
        </p>
    </footer>

    <!-- Load scripts -->
    <script src="js/utils.js"></script>
    <script src="js/main.js"></script>
</body>
</html> 