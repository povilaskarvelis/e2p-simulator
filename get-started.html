<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="Getting started with E2P Simulator">
    <meta name="viewport" content="width=1400, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>E2P Simulator | Get Started</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VDYP5LLPT5"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VDYP5LLPT5');
    </script>
    
    <!-- Favicon links -->
    <link rel="icon" href="images/favicon.png" type="image/png">
    <link rel="apple-touch-icon" href="images/favicon.png">
    <link rel="canonical" href="https://e2p-simulator.com/get-started.html" />
    
    <!-- Load libraries first -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.1/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <!-- Load styles -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/binary.css">
    <link rel="stylesheet" href="css/continuous.css">
    
    <!-- MathJax for professional equation rendering -->
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.auto.min.js"></script>
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
        },
        svg: {
            scale: 1,
            minScale: .5,
            mtextInheritFont: false,
            merrorInheritFont: true,
            mathmlSpacing: false,
            skipAttributes: {},
            exFactor: .5,
            displayAlign: 'center',
            displayIndent: '0',
            versionWarning: false,
        },
        options: {
            enableMenu: false,
            menuOptions: {
                settings: {
                    zoom: "Click",
                    zscale: "200%",
                }
            }
        }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Header -->
    <header class="site-header">
        <a href="index.html" class="site-title" style="display: inline-flex; align-items: center;">
            <img src="images/favicon.png" alt="E2P Simulator Logo" style="height: 32px; width: 32px; margin-right: 8px;">E2P Simulator
        </a>
        <div class="header-right">
            <nav class="site-nav">
                <a href="index.html" class="nav-link">Home</a>
                <a href="get-started.html" class="nav-link active">Get Started</a>
            </nav>
            <a href="https://github.com/povilaskarvelis/e2p-simulator" target="_blank" rel="noopener noreferrer" class="github-link" aria-label="View source on GitHub">
                <svg viewBox="0 0 98 96" xmlns="http://www.w3.org/2000/svg" class="github-logo"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-2.958.324-2.958 4.93 1.364 7.496 5.032 7.496 5.032 4.38 7.849 11.564 5.571 14.355 4.239.446-3.281 1.711-5.571 3.074-6.84-10.943-1.243-22.478-5.526-22.478-24.283 0-5.378 1.94-9.778 5.021-13.2-.485-1.243-2.11-6.283.484-13.018 0 0 4.125-1.304 13.426 5.032a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.336 13.427-5.032 13.427-5.032 2.594 6.735.97 11.775.486 13.018 3.14 3.422 5.021 8.012 5.021 13.2 0 18.807-11.478 23.015-22.478 24.283 1.79 1.548 3.316 4.481 3.316 9.126 0 6.608-.056 11.897-.056 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="#24292f"/></svg>
            </a>
        </div>
    </header>

    <h1>Getting Started with E2P Simulator</h1>

    <div class="description">
        <p>Welcome to E2P Simulator! This guide will help you understand what it does, <br> why it is needed, and how to use it.</p>
    </div>

    <div class="guide-section">
        <h2>What is E2P Simulator?</h2>
        <p>E2P Simulator (Effect-to-Prediction Simulator) allows researchers to interactively and quantitatively explore the relationship between effect sizes (e.g., Cohen's d, Odds Ratio, Pearson's r) and their predictive utility (e.g., AUC, PR-AUC, MCC), while accounting for measurement reliability and outcome base rates.</p>

        <p>In other words, E2P Simulator is a tool for performing <em>predictive utility analysis</em> - estimating how effect sizes will translate into real-world prediction or what effect sizes are needed to achieve a desired predictive performance. Much like how power analysis tools (such as <a href="https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower" target="_blank">G*Power</a>) help researchers plan for statistical significance, E2P Simulator helps plan for practical significance.</p>

        <p>E2P Simulator has several key applications:</p>
        
        <ul>
            <li><strong>Interpretation of findings</strong>: It helps researchers move beyond arbitrary "small/medium/large" effect size labels and ground the interpretation of their findings in terms of predictive value in specific contexts.</li>
            <li><strong>Research planning</strong>: Being able to easily derive what effect sizes are needed to achieve a desired predictive performance allows researchers to plan their studies more effectively and allocate resources more efficiently.</li>
            <li><strong>Education</strong>: The simulator's interactive design makes it a valuable teaching tool, helping students and researchers develop a more intuitive understanding of how different abstract statistical metrics relate to one another and to real-world utility.</li>
        </ul>
    </div>

    <div class="guide-section">
        <h2>Why is E2P Simulator needed?</h2>
        
        <p>Many research areas such as biomedical, behavioral, education, and sports sciences, are increasingly studying individual differences in order to build predictive models to personalize treatments, learning, and training. Identifying reliable biomarkers and other predictors is central to these efforts. Yet, several entrenched research practices continue to undermine the search for predictors:</p>

        <ul>
            <li><strong>Overemphasis on statistical significance:</strong> Most research continues to optimize for statistical significance (p-values) without optimizing for practical significance (effect sizes).</li>
            <li><strong>Difficulties interpreting effect sizes:</strong> The interpretation of effect sizes, which are critical for gauging real-world utility, is often reduced to arbitrary cutoffs (small/medium/large) without providing any clear sense of their practical utility.</li>
            <li><strong>Overlooked measurement reliability:</strong> Measurement noise attenuates observed effect sizes and weakens predictive power, yet it is rarely accounted for in study design or interpretation.</li>
            <li><strong>Neglected outcome base rates:</strong> Low-prevalence events drastically limit predictive performance in real-world settings, but are often not considered when interpreting findings.</li>
        </ul>

        <p>Together, these issues undermine the quality and impact of academic research, resulting in an abundance of statistically significant but practically negligible findings. Conflating statistical significance with practical significance, researchers develop unrealistic expectations about the potential impact of their findings, leading to inefficient study planning, resource misallocation, and considerable waste of time and funding. For example, collecting large datasets or building complex predictive models is pointless if the effect sizes of individual predictors are known to be small (which is usually the case).</p>

        <p>E2P Simulator is designed to address these fundamental challenges by placing effect sizes, measurement reliability, and outcome base rates at the center of study planning and interpretation. It helps researchers understand how these factors jointly shape predictive utility, and guides them in making more informed decisions about their studies.</p>

    <div class="guide-section">
        <h2>How to use E2P Simulator</h2>

        <p>E2P Simulator is designed to be intuitive and interactive. You can explore different scenarios by adjusting effect sizes, measurement reliability, base rates, and decision threshold, and immediately see how these changes impact predictive performance through various visualizations and metrics. Still, in this section we will highlight and clarify some of the key features and assumptions of the simulator.</p>

        <div id="use-summary-diagram" style="text-align: center; margin: 30px 0;">
            <img src="images/use_summary.png" alt="E2P Simulator overview showing all key inputs and interactive elements at a high level" class="screenshot" style="max-width: 100%; width: 100%;">
        </div>
        
        <p>The image above provides an overview of all E2P Simulator's interactive components.</p>
  
        <h3>Binary vs. Continuous Outcomes</h3>
        <p>E2P Simulator provides two analysis modes that cover the two most common research scenarios:</p>

        <ul>
            <li><strong>Binary Mode</strong>: Considers dichotomous outcomes such as diagnostic categories (e.g., cases vs. controls) or discrete states (e.g., success vs. failure). All metric calculations and conversions in this mode are completely analytical and follow the formulas provided on the page.</li>
            
            <li><strong>Continuous Mode</strong>: Considers continuous measurements such as symptom severity or performance scores that may need to be categorized (e.g., responders vs. non-responders or performers vs. non-performers) for practical decisions. This mode is based on actual data simulations rather than analytical solutions, hence it may be slower to react.</li>
        </ul>
        
        <h3>Measurement Reliability and True vs. Observed Effects</h3>
        <p>Measurement reliability attenuates observed effect sizes, and in turn attenuates predictive performance. The simulator allows you to specify reliability using the Intraclass Correlation Coefficient (ICC) for continuous measurements and Cohen's kappa (κ) for binary classifications. These typically correspond to test-retest reliability and inter-rater reliability, respectively. You can toggle between "true" effect sizes (what would be observed with perfect measurement) and "observed" effect sizes (what we actually see given imperfect reliability).</p>
       
        <p>Note that the simulator does not account for sample size limitations, which can introduce additional uncertainty around the true effect size through sampling error.</p>
        
        <h3>Base Rates</h3>
        <p>The base rate refers to how common an outcome is within a specific population. For instance, if an effect size measures the difference between a group with a disorder and a healthy control group, the base rate would be the disorder's prevalence in the general population. However, if you're focusing on a pre-identified high-risk group, the relevant base rate becomes the disorder's prevalence <em>within this specific high-risk cohort</em>. In Bayesian statistics, this base rate is analogous to the prior probability of the outcome, established before considering any particular predictor.</p>
            
        <p>The base rate directly affects PPV and NVP, and indirectly the metrics that depend on it: PR-AUC, MCC, and F-1 score (see <a href="#understanding-metrics">Understanding Predictive Metrics</a>).</p>

        <h3>Multivariate Effects Calculators</h3>
        <p>Both binary and continuous outcomes analysis modes include calculators that help explore how multiple predictors can be combined to achieve stronger effects:</p>
        
        <ul>
            <li><strong>Mahalanobis D Calculator</strong> (Binary mode)</li>
            <li><strong>Multivariate R² Calculator</strong> (Continuous mode)</li>
        </ul>
        
        <p>The calculators can help approximate the expected performance of multivariate models without having to train the full models and thus help with research planning and model development. More specifically, they illustrate:</p>
        
        <ul>
            <li>How the number of predictors affects combined effect size</li>
            <li>The diminishing returns of adding more predictors</li>
            <li>How collinearity (correlation among predictors) reduces their combined effectiveness</li>
            <li>The trade-offs between using fewer strong predictors versus more moderate ones</li>
        </ul>     

        <h4>Calculator Assumptions and Limitations</h4>
        <p>Both calculators correspond to fundamental predictive models in statistics: the Mahalanobis D Calculator approximates Linear Discriminant Analysis and logistic regression, while the Multivariate R² Calculator aligns with multiple linear regression.</p>
        
        <p>Like the statistical methods they approximate, the calculators operate under several simplifying assumptions:</p>
        <ul>
            <li><strong>Average effects and correlations</strong>: The calculators use single values to represent the average effect size across predictors and average correlation among them, which can provide useful approximations even when individual predictors vary in strength</li>
            <li><strong>Linear effects</strong>: The formulas assume predictors contribute additively without interactions (where one predictor's effect depends on another). However, research shows that in clinical prediction, complex non-linear models generally do not outperform simple linear logistic regression (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0895435618310813" target="_blank">Christodoulou et al., 2019</a>)</li>
            <li><strong>Normality</strong>: The underlying variables are assumed to be normally distributed. This is consistent with the assumptions of input metrics like Cohen's d and Pearson's r, although in practice these are often computed despite normality violations.</li>
        </ul>
        
        <p>Despite these limitations, these calculators serve as valuable tools for building intuition about how multiple predictors combine to achieve stronger effects. Even though real-world predictors will vary in their individual strengths and collinearity, the overall patterns demonstrated (such as diminishing returns and the impact of shared variance) remain informative for understanding multivariate relationships.</p>
    </div>

    <div class="guide-section">
        <h2 id="understanding-metrics">Understanding Predictive Metrics</h2>
        
        <h3>Classification Outcomes and Metrics</h3>
        <p>When using a predictor to classify cases into two groups, there are four possible outcomes: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). These form the basis for all predictive metrics.</p>
        
        <div id="predictive-metrics-diagram" style="text-align: center; margin: 20px 0;">
            <img src="images/prediction_metrics.png" alt="Prediction metrics diagram showing confusion matrix and all derived metrics" class="screenshot" style="max-width: 90%;">
        </div>
        
        <p>The image above illustrates how these four outcomes relate to various classification metrics. On the left, you can see how negative (e.g., controls) and positive (e.g., cases) distributions overlap and how a classification threshold (red line) creates these four outcomes. On the right, you'll find the confusion matrix and the formulas for key metrics derived from it. Note how some metrics have multiple names (e.g., Sensitivity/Recall/TPR, Precision/PPV) - this reflects how the same concepts are referred to differently across fields like medicine, cognitive science, and machine learning.</p>
        
        <h3>Key Metrics for Different Contexts</h3>
        <p>Depending on your research context, certain metrics may be more relevant than others:</p>
        <ul>
            <li><strong>When false negatives are costly</strong> (e.g., missing a disease diagnosis): Focus on Sensitivity/Recall. In clinical settings where missing a diagnosis could be life-threatening, maximizing Sensitivity ensures fewer cases are missed, even if it means more false alarms.</li>
            <li><strong>When false positives are costly</strong> (e.g., unnecessary treatments): Focus on Specificity. When treatments have significant side effects or costs, high Specificity ensures fewer healthy individuals receive unnecessary interventions.</li>
            <li><strong>When dealing with low base rate</strong>: Focus on Precision (PPV), NPV, F1 score, MCC, and PR-AUC. These metrics are sensitive to the base rate and thus provide a more accurate assessment of how a predictor would perform in the real world.</li>
        </ul>
        
        <h3 id="threshold-metrics">Threshold-Independent Metrics</h3>
        
        <div id="aucs-diagram" style="float: right; margin: 0 0 20px 20px;">
            <img src="images/aucs.png" alt="ROC and Precision-Recall curves showing AUC calculation" class="screenshot" style="max-width: 350px; width: 350px;">
        </div>
        
        <p>Some metrics evaluate performance across all possible thresholds and can serve as a better summary of the overall model performance. These include:</p>
        <ul>
            <li><strong>ROC-AUC (Area Under the Receiver Operating Characteristic Curve)</strong>: Visualizes how well a model balances true positives (Sensitivity) and false positives (1-Specificity) across all possible thresholds, capturing the trade-off between the two. An AUC of 0.5 means the model is no better than flipping a coin, while 1.0 means perfect separation between groups.</li>
            <li><strong>PR-AUC (Area Under the Precision-Recall Curve)</strong>: Shows how well a model can maintain both precision (PPV) and Recall (Sensitivity) together, which is particularly informative in the context of rare outcomes, as the base rate affects Precision. A larger PR-AUC means the model can achieve high Precision without sacrificing Recall (or vice versa), indicating a smaller trade-off between finding all positive cases and avoiding false alarms.</li>
        </ul>
        
        <p>Both ROC-AUC and PR-AUC represent areas under their respective curves and are mathematically expressed as integrals:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[ROC\text{-}AUC = \int_0^1 TPR(FPR) \, d(FPR)\]
        </div>
    
        <div style="text-align: center; margin: 15px 0;">
            \[PR\text{-}AUC = \int_0^1 P(R) \, dR\]
        </div>
        <p>Where P is Precision (PPV) and R is Recall (Sensitivity). These integrals are computed using trapezoidal numerical integration.</p>
        
        <h3 id="dca-analysis">Decision Curve Analysis (DCA)</h3>
        <p>Decision Curve Analysis (<a href="https://pubmed.ncbi.nlm.nih.gov/17099194/" target="_blank">Vickers & Elkin, 2006</a>) evaluates the clinical utility of a predictive model or a single predictor by plotting net benefit across different threshold probabilities. Unlike ROC and PR curves that focus on discrimination, DCA incorporates the relative costs of false positives and false negatives, helping determine whether using a prediction model to guide treatment decisions provides more benefit than simple strategies.</p>
        
        <div id="dca-diagram" style="float: right; margin: 0 0 20px 20px;">
            <img src="images/dca.png" alt="Decision Curve Analysis example showing net benefit curves and shaded areas representing different strategies" class="screenshot" style="max-width: 400px; width: 400px;">
        </div>
        
        <p>A DCA plot typically includes three key curves:</p>
        <ul>
            <li><strong>Predictor Curve</strong>: Shows the net benefit of using the at different threshold probabilities. The curve's height indicates how much benefit the predictor provides compared to treating no one.</li>
            <li><strong>Treat All</strong>: Represents the strategy of treating everyone regardless of their predicted risk. This strategy maximizes sensitivity (no false negatives) but results in many unnecessary treatments (false positives).</li>
            <li><strong>Treat None</strong>: Represents the strategy of treating no one, which always yields zero net benefit but avoids all treatment-related harms.</li>
        </ul>
        
        <p>The <strong>net benefit</strong> formula accounts for both the benefits of true positives and the costs of false positives:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[NB = \frac{TP}{N} - \frac{FP}{N} \times \frac{P_t}{1-P_t}\]
        </div>
        <p>Where P<sub>t</sub> is the threshold probability, representing the odds at which a patient would be willing to accept treatment. In practical terms, P<sub>t</sub> reflects how many unnecessary interventions one is willing to accept to prevent one adverse outcome. The ratio P<sub>t</sub>/(1-P<sub>t</sub>) captures the relative weighting of false positives compared to true positives.</p>
        
        <p><strong>A-NBC (Area Under the Net Benefit Curve)</strong> summarizes the overall clinical utility of a model across a range of threshold probabilities (<a href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-016-0336-x" target="_blank">Talluri & Shete, 2016</a>). It is calculated using trapezoidal integration:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[A\text{-}NBC = \int_{P_{t1}}^{P_{t2}} NB(P_t) \, dP_t\]
        </div>
        <p>Where the integration is performed over a clinically relevant range of threshold probabilities (adjustable using the movable gray bars in the simulator). A larger A-NBC indicates greater overall clinical utility within that range.</p>
        
        <p><strong>ΔA-NBC (Delta A-NBC)</strong> represents the additional clinical utility that a predictive model provides compared to the best available simple strategy (either treating all patients or treating none). Rather than comparing only against treating no one, ΔA-NBC is calculated as:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[\Delta A\text{-}NBC = A\text{-}NBC_{\text{model}} - \max(A\text{-}NBC_{\text{treat all}}, A\text{-}NBC_{\text{treat none}})\]
        </div>
        <p>At each threshold probability, the model's net benefit is compared against whichever simple strategy performs better at that threshold. This provides a more conservative and clinically meaningful assessment of the model's added value. A positive ΔA-NBC indicates that the predictive model offers genuine improvement over the best simple strategy, while values near zero suggest that simple strategies may be equally effective.</p>
        
        <p><em>Note:</em> For computational simplicity, the ΔA-NBC calculation in this simulator assumes a uniform distribution of threshold probabilities across the selected range. In practice, the distribution of clinically relevant threshold probabilities may be non-uniform, which could affect the relative weighting of different threshold ranges in the overall utility assessment (<a href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-016-0336-x" target="_blank">Talluri & Shete, 2016</a>).</p>
        
        <p>DCA is particularly valuable because it:</p>
        <ul>
            <li>Incorporates clinical context through threshold probabilities that reflect real decision-making scenarios</li>
            <li>Accounts for the relative costs of different types of errors (false positives vs. false negatives)</li>
            <li>Provides actionable insights about when a model should or should not be used clinically</li>
            <li>Facilitates comparison between different models or strategies across various clinical contexts</li>
        </ul>

        <p>For more information about DCA, visit <a href="https://mskcc-epi-bio.github.io/decisioncurveanalysis" target="_blank">https://mskcc-epi-bio.github.io/decisioncurveanalysis</a>.</p>

       </div>

       <div class="guide-section">
        <h2>Quick Start Examples</h2>
        
        To further clarify how the tool can be used and to demonstrate its utility we provide some specific examples.

        <h3>Example 1: Diagnostic Prediction</h3>
        <p> Can we predict depression diagnosis using a cognitive biomarker?  </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 8% (the prevalence of depression in the population; <a href="https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bjc.12333" target="_blank">Shorey et al., 2022</a>)</li>
                    <li>Set the grouping reliability to 0.28 (depression diagnosis reliability based on DSM-5 field trials; <a href="https://pubmed.ncbi.nlm.nih.gov/23111466/" target="_blank">Regier et al., 2013</a>)</li>
                    <li>Set the predictor reliability for both groups to 0.6 (an average reliability for cognitive measures; <a href="https://www.sciencedirect.com/science/article/pii/S0149763423001069" target="_blank">Karvelis et al., 2023</a>)</li>
                    <li>Set the observed effect size to d = 0.8 (a large effect size that is optimistic and rarely seen in practice)</li>
                </ul>
            </li>

            <li>This will yield ROC-AUC = 0.71 and PR-AUC = 0.19. Even with the optimistic effect size of 0.8, the predictive utility remains modest, particularly in balancing recall and precision (as shown by the low PR-AUC). The DCA plot also suggests limited clinical utility, with predictor-based diagnosis showing minimal improvement over diagnosing no one / everyone within the 5% to 30% threshold probability range (ΔA-NBC = 0.001).</li>

            <li>Note that with the low reliability values, this observed effect corresponds to a much larger true effect, d = 1.58, and much better predictive utility, ROC-AUC = 0.87, PR-AUC = 0.46, and ΔA-NBC = 0.006. This demonstrates the importance of measurement reliability in predictive modeling. </li>

            <li>Now let's say we are serious about precision psychiatry and we want to achieve a PR-AUC of 0.8. Using the tool, we can find that it would require d = 2.55. It would be rather unrealistic to expect a single biomarker to achieve this effect size. Using the Mahalanobis D calculator, you can explore how many predictors with smaller d values would be required to achieve D = 2.55.</li>

        </ol>
        
        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=binary&baseRate=8&groupingReliability=0.28&predictorReliabilityGroup1=0.6&predictorReliabilityGroup2=0.6&trueEffectSize=1.58&label1=Controls&label2=Depression&xaxisLabel=Cognitive%20marker&thresholdValue=1.7'">Explore this example yourself →</button>
        

        <h3>Example 2: Treatment Response Prediction</h3>
        <p> Can we predict who will respond to antidepressant treatment using task-based brain activity measures?</p>
        <ol>
            <li>Select Continuous outcome mode:</li>
                <ul>
                    <li>Set base rate to 15% (the rate of response to antidepressant treatment beyond placebo; <a href="https://www.bmj.com/content/378/bmj-2021-067606" target="_blank">Stone et al., 2022</a>)</li>
                    <li>Set predictor reliability to 0.4 (an average reliability for task-fMRI measures; <a href="https://journals.sagepub.com/doi/full/10.1177/0956797620916786" target="_blank">Elliott et al., 2020</a>)</li>
                    <li>Set outcome reliability to 0.94 (Hamilton Depression Rating Scale (HAMD) reliability; <a href="https://pubmed.ncbi.nlm.nih.gov/21276619/" target="_blank">Trajković et al., 2011</a>)</li>
                    <li>Adjust effect size such that R² = 0.2 (average multivariate R² from recent research; <a href="https://direct.mit.edu/netn/article/6/4/1066/109196/Computational-approaches-to-treatment-response" target="_blank">Karvelis et al., 2022</a>)</li>
                </ul>

            <li>This will yield AUC = 0.73 and PR-AUC = 0.32, indicating rather modest predictive performance, as shown by the low PR-AUC. The clinial utility would also be rather limited wtih ΔA-NBC = 0.004 within 5%-30% threshold probability range, as indicated by the DCA plot. Improving measurement reliability could improve the performance up to AUC = 0.87 and PR-AUC = 0.57, and clinical utility up to ΔA-NBC = 0.014, which is a substantial improvement.</li>

            <li>If we want to once again be serious about precision psychiatry and aim for PR-AUC of 0.8, we will find it requires r = 0.9 (R² = 0.81). These are rather extremely ambitious values (requiring to explain 81% of variance in symptom improvement). This helps demonstrate the inherent limitations of dichotomizing continuous outcomes for evaluating treatment response prediction - doing so leads to a loss of valuable information. On the other hand, it does reflect the binary nature of decision-making in psychiatry (to prescribe the treatment or not).</li>
        </ol>

        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=continuous&predictorReliability=0.4&outcomeReliability=0.94&effectSizeR=0.73&baseRate=15&label1=Non-responders&label2=Responders&xaxisLabel=Task-fMRI%20marker&yaxisScatterLabel=Treatment%20response&thresholdValue=0.5'">Explore this example yourself →</button>

        <h3>Example 3: Risk Prediction</h3>
        <p> Can we predict who will attempt suicide using a combination of risk factors?</p> 
            
        <p>Here we will rely mostly on <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3000886/" target="_blank">Borges et al., 2011</a>, who analyzed the data of 108,705 adults from 21 countries, and using a logistic regression model found that combining a range of risk factors (socio-demographics, parent psychopathology, childhood adversities, DSM-IV disorders, and history of suicidal behavior) led to AUC = 0.74-0.8 discrimination performance between those who did and did not attempt suicide. </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 0.3% (the 12-month prevalence of suicide attempts; <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3000886/" target="_blank">Borges et al., 2011</a>)</li>
                    <li>Set the grouping reliability to 0.8 (<a href="https://cssrs.columbia.edu/wp-content/uploads/CSSRS_Supporting-Evidence_Book_2022.pdf" target="_blank">The Columbia Suicide Severity Rating Scale (C-SSRS) supporting evidence</a>)</li>
                    <li>Set the predictor reliability for both groups to 0.8 (a rough estimate for the average reliability across the risk factors) </li>
                    <li>Set the observed effect size to d = 1.18, which corresponds to AUC = 0.8</li>
                </ul>
            </li>

            <li>This will yield PR-AUC = 0.02, indicating extremely abysmal predictive performance, meaning that depending on where we place the threshold, we would either predict a lot of false positives (low precision, high recall) or a lot of false negatives (high precision, low recall). When the threshold is set to maximize the F1 score (0.06), we get precision = 0.06 and recall = 0.06. Which means that we would miss 94% of actual attempters and 94% of the predicted attempters would not attempt suicide. Not surprisingly, this give us extremely low clinical utility (ΔA-NBC = 0.000).</li>

            <li>Considering that we are serious about precision psychiatry and want to aim for PR-AUC of 0.8, we will find it requires observed d = 3.76, which means we have a very long way to go. Note that because the reliability is already quite high, improving it further would not make much of a difference. What we need to do is to find better predictors. Alternatively, it may be more effective to simply focus on universal suicide prevention strategies rather than trying to predict individual cases (e.g.,<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6296389/" target="_blank">Large 2018</a>).</li>
        </ol>

        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=binary&baseRate=0.3&groupingReliability=0.8&predictorReliabilityGroup1=0.8&predictorReliabilityGroup2=0.8&trueEffectSize=1.35&label1=Controls&label2=Suicide%20attempters&xaxisLabel=Predictor&thresholdValue=3.1'">Explore this example yourself →</button>
   
    </div>

    <div class="guide-section">
        <h2>Feedback and Contributions</h2>
        <p>E2P Simulator is an open-source project - feedback, bug reports, and suggestions for improvement are welcome. The easiest way to do so is through the <a href="https://github.com/povilaskarvelis/e2p-simulator/issues" target="_blank">GitHub Issues page</a>.</p>
        <p>You can view the source code, track development, and contribute directly at the <a href="https://github.com/povilaskarvelis/e2p-simulator" target="_blank">project's GitHub repository</a>.</p>
        <p>For other inquiries, you can find my contact information <a href="https://povilaskarvelis.github.io/" target="_blank">here</a>.</p>
    </div>

    <!-- Footer -->
    <footer style="text-align: center; padding: 20px; margin-top: 40px; border-top: 1px solid #eee;">
        <p style="color: #666; font-size: 0.9em;">
            E2P Simulator is open source software. 
            <a href="https://github.com/povilaskarvelis/e2p-simulator" style="color: #0366d6; text-decoration: none;">View on GitHub</a> | 
            <a href="https://github.com/povilaskarvelis/e2p-simulator/blob/main/LICENSE" style="color: #0366d6; text-decoration: none;">MIT License</a>
        </p>
        <p style="color: #666; font-size: 0.9em; margin-top: 10px;">
            Developed by <a href="https://povilaskarvelis.github.io/" style="color: #0366d6; text-decoration: none;" target="_blank">Povilas Karvelis</a>
        </p>
    </footer>

    <!-- Load scripts -->
    <script src="js/utils.js"></script>
    <script src="js/dca.js"></script>
    <script src="js/main.js"></script>
</body>
</html> 