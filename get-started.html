<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="Getting started with E2P Simulator">
    <meta name="viewport" content="width=1400, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>E2P Simulator | Get Started</title>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VDYP5LLPT5"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VDYP5LLPT5');
    </script>
    
    <!-- Favicon links -->
    <link rel="icon" href="images/favicon.png" type="image/png">
    <link rel="apple-touch-icon" href="images/favicon.png">
    <link rel="canonical" href="https://e2p-simulator.com/get-started.html" />
    
    <!-- Load libraries first -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/10.6.1/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <!-- Load styles -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/binary.css">
    <link rel="stylesheet" href="css/continuous.css">
    
    <!-- MathJax for professional equation rendering -->
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/es6-promise@4/dist/es6-promise.auto.min.js"></script>
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
        },
        svg: {
            scale: 1,
            minScale: .5,
            mtextInheritFont: false,
            merrorInheritFont: true,
            mathmlSpacing: false,
            skipAttributes: {},
            exFactor: .5,
            displayAlign: 'center',
            displayIndent: '0',
            versionWarning: false,
        },
        options: {
            enableMenu: false,
            menuOptions: {
                settings: {
                    zoom: "Click",
                    zscale: "200%",
                }
            }
        }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Header -->
    <header class="site-header">
        <a href="index.html" class="site-title" style="display: inline-flex; align-items: center;">
            <img src="images/favicon.png" alt="E2P Simulator Logo" style="height: 32px; width: 32px; margin-right: 8px;">E2P Simulator
        </a>
        <div class="header-right">
            <nav class="site-nav">
                <a href="index.html" class="nav-link">Home</a>
                <a href="get-started.html" class="nav-link active">Get Started</a>
            </nav>
            <a href="https://github.com/povilaskarvelis/e2p-simulator" target="_blank" rel="noopener noreferrer" class="github-link" aria-label="View source on GitHub">
                <svg viewBox="0 0 98 96" xmlns="http://www.w3.org/2000/svg" class="github-logo"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-2.958.324-2.958 4.93 1.364 7.496 5.032 7.496 5.032 4.38 7.849 11.564 5.571 14.355 4.239.446-3.281 1.711-5.571 3.074-6.84-10.943-1.243-22.478-5.526-22.478-24.283 0-5.378 1.94-9.778 5.021-13.2-.485-1.243-2.11-6.283.484-13.018 0 0 4.125-1.304 13.426 5.032a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.336 13.427-5.032 13.427-5.032 2.594 6.735.97 11.775.486 13.018 3.14 3.422 5.021 8.012 5.021 13.2 0 18.807-11.478 23.015-22.478 24.283 1.79 1.548 3.316 4.481 3.316 9.126 0 6.608-.056 11.897-.056 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="#24292f"/></svg>
            </a>
        </div>
    </header>

    <h1>Getting Started with E2P Simulator</h1>

    <div class="description">
        <p>Welcome to E2P Simulator! This guide will help you understand what it does, <br> why it is needed, and how to use it.</p>
    </div>

    <div class="guide-section">
        <h2>What is E2P Simulator?</h2>
        <p>E2P Simulator (Effect-to-Prediction Simulator) allows researchers to interactively and quantitatively explore the relationship between effect sizes (e.g., Cohen's d, Odds Ratio, Pearson's r), the corresponding predictive performance (e.g., ROC-AUC, Sensitivity, Specificity, Accuracy, etc.), and real-world predictive utility (e.g., PPV, NPV, PR-AUC, Net Benefit, etc.) by accounting for measurement reliability and outcome base rates.</p>

        <p>In other words, E2P Simulator is a tool for performing <em>predictive utility analysis</em> - estimating how research findings will translate into real-world prediction or what effect sizes/predictive performance is needed to achieve a desired level of predictive and clinical utility. Much like how power analysis tools (such as <a href="https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower" target="_blank">G*Power</a>) help researchers plan for statistical significance, E2P Simulator helps plan for practical significance.</p>

        <p>E2P Simulator has several key applications:</p>
        
        <ul>
            <li><strong>Interpretation of findings</strong>: It helps researchers move beyond arbitrary "small/medium/large" effect size labels and misleading predictive metrics by grounding their interpretation in estimated real-world predictive utility.</li>
            <li><strong>Research planning</strong>: Being able to easily derive what effect sizes and predictive performance are needed to achieve a desired real-world predictive performance allows researchers to plan their studies more effectively and allocate resources more efficiently.</li>
            <li><strong>Education</strong>: The simulator's interactive design makes it a valuable teaching tool, helping researchers develop a more intuitive understanding of how different abstract statistical metrics relate to one another and to real-world utility.</li>
        </ul>
    </div>

    <div class="guide-section">
        <h2>Why is E2P Simulator needed?</h2>
        
        <p>Many research areas such as biomedical, behavioral, education, and sports sciences are increasingly studying individual differences to build predictive models to personalize treatments, learning, and training. Identifying reliable biomarkers and other predictors is central to these efforts. Yet, several entrenched research practices continue to undermine the search for predictors:</p>

        <ul>
            <li><strong>Overemphasis on statistical significance:</strong> Most research continues to optimize for statistical significance (p-values) without optimizing for practical significance (effect sizes).</li>
            <li><strong>Difficulty interpreting effect sizes:</strong> The interpretation of effect sizes, which are critical for gauging real-world utility, is often reduced to arbitrary cutoffs (small/medium/large) without conveying their practical utility.</li>
            <li><strong>Overlooked measurement reliability:</strong> Measurement noise attenuates both effect sizes and predictive performance, yet it is rarely accounted for in study design or interpretation of findings.</li>
            <li><strong>Neglected outcome base rates:</strong> Low prevalence of outcomes can drastically limit predictive performance in real-world settings, but is rarely accounted for when evaluating the translational potential of prediction models.</li>
        </ul>

        <p>Together, these issues undermine the quality and impact of academic research, because routinely reported metrics do not reflect real-world utility. Whether researchers focus on achieving statistical significance of individual predictors or optimizing model performance metrics like accuracy and ROC-AUC, both approaches often lead to unrealistic expectations about practical impact. In turn, this results in inefficient study planning, resource misallocation, and considerable waste of time and funding.</p>

        <p>E2P Simulator is designed to address these fundamental challenges by placing measurement reliability and outcome base rates at the center of study planning and interpretation. It helps researchers understand how these factors jointly shape real-world predictive utility, and guides them in making more informed research decisions.</p>

    <div class="guide-section">
        <h2>How to use E2P Simulator</h2>

        <p>E2P Simulator is designed to be intuitive and interactive. You can explore different scenarios by adjusting effect sizes, measurement reliability, base rates, and decision threshold, and immediately see how these changes impact predictive performance through various visualizations and metrics. Still, in this section we will highlight and clarify some of the key features and assumptions of the simulator.</p>

        <div id="use-summary-diagram" style="text-align: center; margin: 30px 0;">
            <img src="images/use_summary.png" alt="E2P Simulator overview showing all key inputs and interactive elements at a high level" class="screenshot" style="max-width: 100%; width: 100%;">
        </div>
        
        <p>The image above provides an overview of all E2P Simulator's interactive components.</p>
  
        <h3>Binary vs. Continuous Outcomes</h3>
        <p>E2P Simulator provides two analysis modes that cover the two most common research scenarios:</p>

        <ul>
            <li><strong>Binary Mode</strong>: Considers dichotomous outcomes such as diagnostic categories (e.g., cases vs. controls) or discrete states (e.g., success vs. failure). All metric calculations and conversions in this mode are completely analytical and follow the formulas provided on the page.</li>
            
            <li><strong>Continuous Mode</strong>: Considers continuous measurements such as symptom severity or performance scores that may need to be categorized (e.g., responders vs. non-responders or performers vs. non-performers) for practical decisions. This mode is based on actual data simulations rather than analytical solutions, hence it may be slower to react.</li>
        </ul>
        
        <h3>Measurement Reliability and True vs. Observed Effects</h3>
        <p>Measurement reliability attenuates observed effect sizes, which in turn reduces predictive performance. The simulator allows you to specify reliability using the Intraclass Correlation Coefficient (ICC) for continuous measurements and Cohen's kappa (κ) for binary classifications. These typically correspond to test-retest reliability and inter-rater reliability, respectively. You can toggle between "true" effect sizes (what would be observed with perfect measurement) and "observed" effect sizes (what we actually see given imperfect reliability).</p>
        
        <p>For continuous outcomes, the relationship between true and observed Pearson's r is given by:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[r_{\text{observed}} = r_{\text{true}} \times \sqrt{ICC_{\text{predictor}} \times ICC_{\text{outcome}}}\]
        </div>

        <p>For binary outcomes, the relationship between true and observed Cohen's d is:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[d_{\text{observed}} = d_{\text{true}} \times \sqrt{\frac{2 \times ICC_1 \times ICC_2}{ICC_1 + ICC_2} \times \sin(\frac{\pi}{2} \kappa)}\]
        </div>

        <p>where \(ICC_1\) and \(ICC_2\) denote reliability of the predictor in each of the two groups, and \(\kappa\) (kappa) is the reliability of the predicted binary outcome labels (e.g., interrater reliability of a diagnosis).</p>

        <p>See <a href="https://osf.io/preprints/psyarxiv/z4yqe_v5" target="_blank">Karvelis & Diaconescu (2025)</a> for more details on how reliability attenuates individual and group differences.</p>
       
        <p>Note that the simulator does not account for sample size limitations, which can introduce additional uncertainty around the true effect size through sampling error.</p>
        
        <h3>Base Rates</h3>
        <p>Base rate (or prevalence) refers to the proportion of individuals in the population who have the outcome of interest before considering any predictors or test results (in Bayesian terms, this is the prior probability of the outcome). To estimate real-world predictive utility, the base rate should be set to reflect the population where your predictor or model will actually be used — not the composition of your study sample. This distinction is crucial because research studies often use case-control designs with balanced sampling (e.g., 50% cases, 50% controls) that do not reflect real-world prevalence. This is one of the most commonly overlooked problems in evaluating prediction models (<a href="https://link.springer.com/chapter/10.1007/978-3-030-50423-6_6" target="_blank">Barbec et al., 2020</a>), as the base rate directly affects multiple metrics used for model evaluation (see <a href="#understanding-metrics">Understanding Predictive Metrics</a>).</p>
            
        <p>For instance, if you are developing a model for a rare disorder that affects 2% of the general population, the base rate should be set to 2%, even if your training dataset contains equal numbers of cases and controls. However, if your model will be used in a pre-screened high-risk population where the disorder prevalence is 20%, then 20% becomes the relevant base rate (however, in this scenario, the effect size should also reflect the difference between cases and high-risk controls rather than general population controls).</p>        

        <h3>Multivariable Simulators</h3>

        <p>Both binary and continuous outcomes analysis modes include simulators that help estimate how many predictors need to be combined to achieve a desired level of real-world predictive utility. The main metric for this is PR-AUC - it accounts for the base rate and is threshold-independent. For binary classification, the simulator also displays Mahalanobis D, a multivariate generalization of Cohen's d, and for continuous outcomes, it displays total variance explained R².</p>
        
        <p>The multivariable simulators can help approximate the expected performance of multivariate models without having to train the full models and thus help with research planning and model development. They also help gain intuition about how multicollinearity undermines predictive performance and leads to diminishing returns when adding more predictors.</p>

        <h4>Assumptions and Limitations</h4>
        
        <p> The multivariable simulators are based on several simplifying assumptions:</p>
        <ul>
            <li><strong>Average effects and correlations</strong>: The simulators use single values to represent the average effect size across predictors and average correlation among them, which can provide useful approximations even when individual predictors vary in strength</li>
            <li><strong>Linear effects</strong>: The formulas assume predictors contribute additively without interactions (where one predictor's effect depends on another). This assumption is supported by research showing that in clinical prediction, complex non-linear models generally do not outperform simple linear logistic regression (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0895435618310813" target="_blank">Christodoulou et al., 2019</a>). In general, more complex machine learning models excel at capturing non-linear relationships that we expect to see in the real world, but they also require more data and are more prone to overfitting.</li>
            <li><strong>Normality</strong>: The underlying variables are assumed to be normally distributed. This is consistent with the assumptions of input metrics like Cohen's d and Pearson's r, although in practice these are often computed despite normality violations.</li>
        </ul>
        
        <p>Even though real-world predictors will often not be normally distributed and will vary in their individual strengths and collinearity, the general trends (such as diminishing returns and the impact of shared variance among the predictors) remain informative for understanding multivariate relationships and estimating expected model performance.</p>

    </div>

    <div class="guide-section">
        <h2 id="understanding-metrics">Understanding Predictive Metrics</h2>
        
        <h3>Classification Outcomes and Metrics</h3>
        <p>When using a predictor to classify cases into two groups, there are four possible outcomes: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). These form the basis for all predictive metrics.</p>
        
        <div id="predictive-metrics-diagram" style="text-align: center; margin: 20px 0;">
            <img src="images/prediction_metrics.png" alt="Prediction metrics diagram showing confusion matrix and all derived metrics" class="screenshot" style="max-width: 100%;">
        </div>
        
        <p>The image above illustrates how these four outcomes are used to derive classification metrics. On the left, you can see how negative (e.g., controls) and positive (e.g., cases) distributions overlap and how a classification threshold (red line) creates these four outcomes. On the right, you can see the confusion matrix and the formulas for key metrics derived from it. Note how some metrics have multiple names (e.g., Sensitivity/Recall/TPR, Precision/PPV) - this reflects how the same concepts are referred to differently across fields like medicine, cognitive science, and machine learning.</p>
        
        <p>A short summary of each metric:</p>
        <ul>
            <li><strong>Sensitivity (Recall, True Positive Rate)</strong>: Measures the proportion of actual positives correctly identified. Useful when missing a positive case is costly (e.g., disease screening), but ignores false positives.</li>
            <li><strong>Specificity (True Negative Rate)</strong>: Measures the proportion of actual negatives correctly identified. Important when false alarms are costly (e.g., confirming a diagnosis before a risky treatment), but ignores false negatives.</li>
            <li><strong>Accuracy</strong>: The proportion of all predictions that are correct. Intuitive but can be misleading for imbalanced datasets, as it is heavily influenced by the majority class.</li>
            <li><strong>Balanced Accuracy</strong>: The average of Sensitivity and Specificity. A better measure than accuracy for imbalanced datasets, but it gives equal weight to both types of errors and does not account for the base rate, which is critical for assessing real-world utility.</li>
            <li><strong>Positive Predictive Value (PPV, Precision)</strong>: The proportion of positive predictions that are actually correct. It informatively accounts for base rate, and corresponds to the posterior probability of a condition given a positive test result ("How likely is a positive prediction to be true?"). As such, it is a crucial metric for clinical decision-making.</li>
            <li><strong>Negative Predictive Value (NPV)</strong>: The proportion of negative predictions that are actually correct. It also informatively accounts for the base rate and corresponds to the posterior probability of not having a condition given a negative test result ("How likely is a negative prediction to be true?"). This makes it a crucial metric for ruling out conditions.</li>
            <li><strong>F1 Score</strong>: The harmonic mean of Precision and Recall. A useful summary when you need to balance finding all positives and not making too many false alarms, though it can be hard to interpret directly.</li>
            <li><strong>Matthews Correlation Coefficient (MCC)</strong>: A correlation between observed and predicted classifications. A balanced measure suitable for imbalanced datasets, but it is less intuitive to interpret and does not reveal the types of errors being made.</li>
        </ul>
        
        <h3 id="threshold-metrics">Threshold-Independent Metrics</h3>
        
        <div id="aucs-diagram" style="float: right; margin: 0 0 20px 20px;">
            <img src="images/aucs.png" alt="ROC and Precision-Recall curves showing AUC calculation" class="screenshot" style="max-width: 350px; width: 350px;">
        </div>
        
        <p>Some metrics evaluate performance across all possible thresholds and can serve as a better summary of the overall model performance. These include:</p>
        <ul>
            <li><strong>ROC-AUC (Area Under the Receiver Operating Characteristic Curve)</strong>: Summarizes how well a model balances true positives (Sensitivity) and false positives (1-Specificity) across all possible thresholds. While useful, ROC-AUC can be misleadingly optimistic for imbalanced datasets (e.g., rare diseases) because it does not account for the base rate of the outcome, which is a critical component of real-world performance.</li>
            <li><strong>PR-AUC (Area Under the Precision-Recall Curve)</strong>: Summarizes how well a model maintains both Precision (PPV) and Recall (Sensitivity). PR-AUC is often a more informative metric for real-world applications because it directly evaluates the trade-off between finding positive cases (Recall) and ensuring that positive predictions are correct (Precision), which critically depends on the outcome's base rate. A high PR-AUC indicates strong performance in practical scenarios where correctly identifying positive cases is paramount. Therefore, to get a realistic estimate of real-world utility, it is essential to use PR-AUC with a base rate that reflects the intended population.</li>
        </ul>
        
        <p>Both ROC-AUC and PR-AUC represent areas under their respective curves and are mathematically expressed as integrals:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[ROC\text{-}AUC = \int_0^1 TPR(FPR) \, d(FPR)\]
        </div>
    
        <div style="text-align: center; margin: 15px 0;">
            \[PR\text{-}AUC = \int_0^1 PPV(TPR) \, d(TPR)\]
        </div>
        <p>These integrals are computed using trapezoidal numerical integration.</p>
        
        <h3 id="dca-analysis">Decision Curve Analysis (DCA)</h3>
        <p>Decision Curve Analysis (<a href="https://pubmed.ncbi.nlm.nih.gov/17099194/" target="_blank">Vickers & Elkin, 2006</a>) evaluates the clinical utility of a predictive model or a single predictor by explicitly balancing the costs of false positives against the benefits of true positives.</p>
        
        <div id="dca-diagram" style="float: right; margin: 0 0 20px 20px;">
            <img src="images/dca.png" alt="Decision Curve Analysis example showing net benefit curves and shaded areas representing different strategies" class="screenshot" style="max-width: 400px; width: 400px;">
        </div>
        
        <p>A DCA plot typically includes three key curves:</p>
        <ul>
            <li><strong>Model</strong>: Shows the net benefit of using the model (or a single predictor) at different threshold probabilities. </li>
            <li><strong>All</strong>: Represents the strategy of intervening for everyone regardless of their predicted risk. This strategy maximizes sensitivity (no false negatives) but results in many unnecessary interventions (false positives).</li>
            <li><strong>None</strong>: Represents the strategy of intervening for no one, which always yields zero net benefit but avoids all intervention-related harms.</li>
        </ul>
        
        <p>The <strong>net benefit</strong> formula accounts for both the benefits of true positives and the costs of false positives:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[NB = \frac{TP}{N_{total}} - \frac{FP}{N_{total}} \times \frac{p_t}{1-p_t}\]
        </div>
        <p>Where N<sub>total</sub> is the total sample size, and p<sub>t</sub> is the threshold probability. It represents the minimum predicted probability of an outcome at which you would decide to intervene (e.g., diagnose or treat). For instance, if p<sub>t</sub> = 0.10, you would intervene for anyone with a predicted risk ≥ 10%. The choice of p<sub>t</sub> determines a specific balance between sensitivity (finding true cases) and specificity (avoiding false alarms). In practical terms, p<sub>t</sub> represents the trade-off between benefits and harms: for every 1/(1-p<sub>t</sub>) - 1 unnecessary interventions you are willing to accept to prevent one adverse outcome. The ratio p<sub>t</sub>/(1-p<sub>t</sub>) in the net benefit formula captures this relative weighting of false positives compared to true positives. The optimal p<sub>t</sub> can be estimated as:</p>

        <div style="text-align: center; margin: 15px 0;">
            \[p_t = \frac{C_{FP}}{C_{FP} + C_{FN}}\]
        </div>

        <p>Where C<sub>FP</sub> is the cost of a false positive (unnecessary intervention) and C<sub>FN</sub> is the cost of a false negative (missed positive case). p<sub>t</sub> can also be estimated through expert surveys, stakeholder preferences, or established guidelines.</p>
        
        <p>For population screening, p<sub>t</sub> is typically set low because missing true cases is costlier than unnecessary follow-ups, so more false positives are acceptable. For diagnostic confirmation (e.g., before initiating high-risk treatment), p<sub>t</sub> is set higher to avoid false positives, reflecting a preference for specificity. As a rule of thumb, screening scenarios may use p<sub>t</sub> in the 1–10% range, whereas diagnostic decisions often warrant much higher p<sub>t</sub> (for example 30–70% or more), depending on harms and preferences.</p>
        
        <p>What we often want to know is not the absolute NB, but added value. <strong>ΔNB (Delta Net Benefit)</strong> measures this additional utility by comparing the model against the better of the two simple strategies (either All or None) at a specific threshold probability:</p>
        <div style="text-align: center; margin: 15px 0;">
            \[\Delta NB = NB_{\text{model}} - \max(NB_{\text{All}}, NB_{\text{None}})\]
        </div>
        
        <p>At each threshold probability, the model's net benefit is compared against whichever simple strategy performs better at that threshold. This provides a more conservative and meaningful assessment of the model's added value. A positive ΔNB indicates that the predictive model offers genuine improvement over the best simple strategy, while values near zero suggest that simple strategies may be equally effective.</p>

        <p>DCA is particularly valuable because it:</p>
        <ul>
            <li>Incorporates the decision-making context through threshold probabilities that reflect real-world scenarios</li>
            <li>Accounts for the relative costs of different types of errors (false positives vs. false negatives)</li>
            <li>Provides actionable insights about when a model should or should not be used</li>
            <li>Facilitates comparison between different models or strategies across various contexts</li>
        </ul>

        <p>For more information about DCA, visit <a href="https://mskcc-epi-bio.github.io/decisioncurveanalysis" target="_blank">https://mskcc-epi-bio.github.io/decisioncurveanalysis</a>.</p>

       </div>

       <div class="guide-section">
        <h2>Quick Start Examples</h2>
        
        To further clarify how the tool can be used and to demonstrate its utility we provide some specific examples.

        <h3>Example 1: Diagnostic Prediction</h3>
        <p> Can we predict depression diagnosis using a cognitive biomarker?  </p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 8% (the prevalence of depression in the population; <a href="https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bjc.12333" target="_blank">Shorey et al., 2022</a>)</li>
                    <li>Set the grouping reliability to 0.28 (depression diagnosis reliability based on DSM-5 field trials; <a href="https://pubmed.ncbi.nlm.nih.gov/23111466/" target="_blank">Regier et al., 2013</a>)</li>
                    <li>Set the predictor reliability for both groups to 0.6 (an average reliability for cognitive measures; <a href="https://www.sciencedirect.com/science/article/pii/S0149763423001069" target="_blank">Karvelis et al., 2023</a>)</li>
                    <li>Set the observed effect size to d = 0.8 (a large effect size that is optimistic and rarely seen in practice)</li>
                </ul>
            </li>

            <li>This will yield ROC-AUC = 0.71 and PR-AUC = 0.19. Even with the optimistic effect size of 0.8, the predictive utility remains modest, particularly in balancing recall and precision (as shown by the low PR-AUC). The DCA plot also suggests limited clinical utility: assuming a threshold probability (p<sub>t</sub>) of 0.1, which would be suitable for a screening scenario, we get ΔNB ≈ 0.018, which means that relying on this biomarker would result in 1.8 additional true positives per 100 patients (after accounting for the harm of false positives at that threshold).</li>

            <li>Note that with the low reliability values, this observed effect corresponds to a much larger true effect, d = 1.58, and much better predictive utility, ROC-AUC = 0.87, PR-AUC = 0.46, and ΔNB ≈ 0.042. This demonstrates the importance of measurement reliability in predictive modeling. </li>

            <li>Now let's say we are serious about precision psychiatry and we want to achieve a PR-AUC of 0.8. Using the tool, we can find that it would require d = 2.55. It would be rather unrealistic to expect a single biomarker to achieve this effect size. Using the Mahalanobis D calculator, you can explore how many predictors with smaller d values would be required to achieve D = 2.55.</li>

        </ol>
        
        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=binary&baseRate=8&groupingReliability=0.28&predictorReliabilityGroup1=0.6&predictorReliabilityGroup2=0.6&trueEffectSize=1.58&label1=Controls&label2=Depression&xaxisLabel=Cognitive%20marker&thresholdValue=1.39'">Explore this example yourself →</button>
        

        <h3>Example 2: Treatment Response Prediction</h3>
        <p> Can we predict who will respond to antidepressant treatment using task-based brain activity measures?</p>
        <ol>
            <li>Select Continuous outcome mode:</li>
                <ul>
                    <li>Set base rate to 15% (the rate of response to antidepressant treatment beyond placebo; <a href="https://www.bmj.com/content/378/bmj-2021-067606" target="_blank">Stone et al., 2022</a>)</li>
                    <li>Set predictor reliability to 0.4 (an average reliability for task-fMRI measures; <a href="https://journals.sagepub.com/doi/full/10.1177/0956797620916786" target="_blank">Elliott et al., 2020</a>)</li>
                    <li>Set outcome reliability to 0.94 (Hamilton Depression Rating Scale (HAMD) reliability; <a href="https://pubmed.ncbi.nlm.nih.gov/21276619/" target="_blank">Trajković et al., 2011</a>)</li>
                    <li>Adjust effect size such that R² = 0.2 (average multivariate R² from recent research; <a href="https://direct.mit.edu/netn/article/6/4/1066/109196/Computational-approaches-to-treatment-response" target="_blank">Karvelis et al., 2022</a>)</li>
                </ul>

            <li>This will yield AUC = 0.73 and PR-AUC = 0.32, indicating rather modest predictive performance, as shown by the low PR-AUC. At p<sub>t</sub> = 0.30, which is a reasonable threshold for accounting for the harms of unnecesarily prescribing antidepressants, we get ΔNB ≈ 0.013. Improving measurement reliability could increase performance up to AUC = 0.87, PR-AUC = 0.57, and ΔNB ≈ 0.054.</li>

            <li>If we want to once again be serious about precision psychiatry and aim for PR-AUC of 0.8, we will find it requires r = 0.9 (R² = 0.81). These are rather extremely ambitious values (requiring to explain 81% of variance in symptom improvement). This helps demonstrate the inherent limitations of dichotomizing continuous outcomes for evaluating treatment response prediction - doing so leads to a loss of valuable information. On the other hand, it does reflect the binary nature of decision-making in psychiatry (to prescribe the treatment or not).</li>
        </ol>

        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=continuous&predictorReliability=0.4&outcomeReliability=0.94&effectSizeR=0.73&baseRate=15&label1=Non-responders&label2=Responders&xaxisLabel=Task-fMRI%20marker&yaxisScatterLabel=Treatment%20response&thresholdValue=0.5'">Explore this example yourself →</button>

        <h3>Example 3: Risk Prediction</h3>
        <p> Can we predict who will attempt suicide using a combination of risk factors?</p> 
            
        <p> Most suicide prediction models (which include a variety of risk factors such as psychopathology, history of suicidal behavior, socio-demographics, etc.) achieve ROC-AUC &gt; 0.7, with a handful exceeding ROC-AUC = 0.9 for predicting suicide attempts (<a href="https://doi.org/10.1038/s41398-024-02852-9" target="_blank">Pigoni et al., 2024</a>). Let's ignore potential overfitting concerns and consider how ROC-AUC = 0.9 would translate to real-world predictive utility.</p>
        <ol>
            <li>In the Binary outcome mode:
                <ul>
                    <li>Set base rate to 0.3% (the 12-month prevalence of suicide attempts; <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3000886/" target="_blank">Borges et al., 2011</a>)</li>
                    <li>Set the grouping reliability to 0.8 (<a href="https://cssrs.columbia.edu/wp-content/uploads/CSSRS_Supporting-Evidence_Book_2022.pdf" target="_blank">The Columbia Suicide Severity Rating Scale (C-SSRS) supporting evidence</a>)</li>
                    <li>Set the predictor reliability for both groups to 0.8 (a rough estimate for the average reliability across the risk factors) </li>
                    <li>Set the observed effect size to d = 2.04, which corresponds to AUC = 0.9</li>
                </ul>
            </li>

            <li>This will yield PR-AUC = 0.08, indicating extremely abysmal predictive performance, meaning that depending on where we place the threshold, we would either predict a lot of false positives (low precision, high recall) or a lot of false negatives (high precision, low recall). When the threshold is set to maximize the F1 score (0.15), we get precision = 0.15 and recall = 0.15. Which means that we would miss 85% of actual attempters and 85% of the predicted attempters would not attempt suicide. Not surprisingly, this gives us extremely low clinical utility (ΔNB ≈ 0.0004 at p<sub>t</sub> = 0.15). At threshold of p<sub>t</sub> = 0.05, which would be reasonable given how costly false negatives (missing actual attempters) are, ΔNB would be very low (≈ 0.0004), illustrating the limited utility of prediction in this context.</li>

            <li>Considering that we are serious about precision psychiatry and want to aim for PR-AUC of 0.8, we will find it requires observed d = 3.76, which means we have a very long way to go. Note that because the reliability is already quite high, improving it further would not make much of a difference. What we need to do is to find better predictors. Alternatively, it may be more effective to simply focus on universal suicide prevention strategies rather than trying to predict individual cases (e.g.,<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6296389/" target="_blank">Large 2018</a>).</li>
        </ol>

        <button class="example-button" style="width: 270px;" onclick="window.location.href='index.html?mode=binary&baseRate=0.3&groupingReliability=0.8&predictorReliabilityGroup1=0.8&predictorReliabilityGroup2=0.8&trueEffectSize=2.04&label1=Controls&label2=Suicide%20attempters&xaxisLabel=Predictor&thresholdValue=3.13'">Explore this example yourself →</button>
   
    </div>

    <div class="guide-section" id="sample-size">
        <h2>Sample Size Calculations for Prediction Models</h2>
        
        <p>E2P Simulator includes sample size calculators that implement evidence-based criteria (<a href="https://www.bmj.com/content/368/bmj.m441" target="_blank">Riley et al., 2020</a>) to ensure your multivariable prediction models have sufficient data to avoid overfitting and ensure low prediction error. These go beyond simple rules of thumb like "10 events per predictor."</p>

        <h3>How to Use</h3>
        <p>Specify your number of predictors, <strong>realistically expected R²</strong> (based on prior research or pilot data), and outcome prevalence (for binary outcomes only). Use R²<sub>CS</sub> for binary outcomes and standard R² for continuous outcomes. Note, for binary outcomes with a single continuous predictor, R²<sub>CS</sub> equals eta-squared (η²), which is already displayed in the main E2P simulator dashboard. The final recommendation uses the <em>maximum</em> across all criteria to ensure all performance targets are met.

        <p>The sample size calculators complement the main E2P simulators in study planning: the E2P simulators explore relationships between effect sizes and predictive utility (both what you need for desired performance and what utility to expect from realistic effects), while the sample size calculators determine adequate sample size based on realistic R² estimates from prior research. For sample size planning, always use conservative, realistic R² estimates based on prior research, not idealized target values.</p>

        <h3>Single Predictor Models and Power Analysis</h3>
        <p>When developing a model with just one predictor (p = 1), the sample size calculations relate directly to traditional power analysis for detecting that predictor's effect. However, the criteria still provide value beyond simple power calculations:</p>
        <ul>
            <li><strong>Power analysis</strong>: Focuses on detecting a statistically significant effect (p &lt; 0.05)</li>
            <li><strong>Prediction-focused criteria</strong>: Ensure the model will be robust and generalizable, with stable coefficients and accurate predictions</li>
        </ul>
        <p>For single predictors, the shrinkage and optimism criteria often require larger sample sizes than traditional power analysis, reflecting the higher standards needed for reliable prediction versus mere statistical significance.</p>

    </div>

    <div class="guide-section">
        <h2>Feedback and Contributions</h2>
        <p>E2P Simulator is an open-source project - feedback, bug reports, and suggestions for improvement are welcome. The easiest way to do so is through the <a href="https://github.com/povilaskarvelis/e2p-simulator/issues" target="_blank">GitHub Issues page</a>.</p>
        <p>You can view the source code, track development, and contribute directly at the <a href="https://github.com/povilaskarvelis/e2p-simulator" target="_blank">project's GitHub repository</a>.</p>
        <p>For other inquiries, you can find my contact information <a href="https://povilaskarvelis.github.io/" target="_blank">here</a>.</p>
    </div>

    <!-- Footer -->
    <footer style="text-align: center; padding: 20px; margin-top: 40px; border-top: 1px solid #eee;">
        <p style="color: #666; font-size: 0.9em;">
            E2P Simulator is open source software. 
            <a href="https://github.com/povilaskarvelis/e2p-simulator" style="color: #0366d6; text-decoration: none;">View on GitHub</a> | 
            <a href="https://github.com/povilaskarvelis/e2p-simulator/blob/main/LICENSE" style="color: #0366d6; text-decoration: none;">MIT License</a>
        </p>
        <p style="color: #666; font-size: 0.9em; margin-top: 10px;">
            Developed by <a href="https://povilaskarvelis.github.io/" style="color: #0366d6; text-decoration: none;" target="_blank">Povilas Karvelis</a>
        </p>
    </footer>

    <!-- Load scripts -->
    <script src="js/utils.js"></script>
    <script src="js/dca.js"></script>
    <script src="js/main.js"></script>
</body>
</html> 